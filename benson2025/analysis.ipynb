{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4066e9b8-fe17-4e89-a968-089b4522f709",
   "metadata": {},
   "source": [
    "# Analysis of the Segmentation of V1, V2, and V3 by Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb435c1-26be-4600-a6f0-1f16e6278c74",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d620c-0b3c-4e5b-a425-de231bc51494",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "[Noah C. Benson](mailto:nben@uw.edu)$^{1}$ and [Bogeng Song](mailto:bs4283@nyu.edu)$^{2,3}$\n",
    "\n",
    "$^1$eScience Institute, University of Washington, Seattle, United States  \n",
    "$^2$Department of Psychology, New York University, New York City, United States  \n",
    "$^3$(Current Affiliation) Department of Psychology, Georgia Institute of Technology, Georgia, United States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39d39cd-b4dc-428f-ac14-6b11686a5ee6",
   "metadata": {},
   "source": [
    "### How to Use this Notebook\n",
    "\n",
    "This notebook contains documentation, analyses, and visualizations that were performed by Benson, Song, et al. (2025) in their report on the segmentation of V1, V2, and V3 using convolutional neural networks. It was published together with a virtual machine, containerized in a docker image, with the intention of providing a persistent means of reproducing the computation in the original paper. To duplicate the environment used in the analysis of the publication, the following instructions are provided:\n",
    "\n",
    "1. **Obtain access to the Human Connectome Project (HCP) data**.\n",
    "    1. **Register at the [HCP connectome database page](https://db.humanconnectome.org/)**.\n",
    "    2. **Obtain and save AWS access credentials**. Once you have an account, log into the database; near the top of the initial splash page is a cell titled \"WU-Minn HCP Data - 1200 Subjects\", and inside this cell is a button for activating Amazon S3 Access. When you activate this feature, you will be given a \"Key\" and a \"Secret\". These should be saved to the file `${HOME}/.aws/credentials` under the heading `[hcp]` where `${HOME}` is your home directory. For more information about the format of this file, see [this page](https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html), but, in brief: if you don't already have a credentials file, you can put the following block of text in it, and if you do already have such a file, you can append the following text to the end of it except with the `______________` replaced with your key and the `********************` replaced with your secret.\n",
    "       ```\n",
    "       [hcp]\n",
    "       aws_access_key_id = ______________\n",
    "       aws_secret_access_key = ********************\n",
    "       ```\n",
    "    3. **Obtain access to the restricted dataset and save the restricted data file**. To obtain access to the restricted data, you need to sign an agreement and send it to the HCP. For more information on restricted data access, see [this page](https://www.humanconnectome.org/study/hcp-young-adult/document/restricted-data-usage). Once your access has been granted, you should receive instructions on obtaining the restricted data CSV file.\n",
    "2. **Set up Docker and obtain the docker image**.\n",
    "    1. **Install and start [docker](https://docker.com/)'s `docker-desktop` service**. See [this page](https://docs.docker.com/get-started/get-docker/) for download and installation instructions. Note that docker requires administrator privileges.\n",
    "    2. **Download the `analysis.tar.gz` docker image from the repository** (DOI:[10.5281/zenodo.14502583](https://doi.org/10.5281/zenodo.14502583)).\n",
    "    3. **Unzip the analysis image file**.\n",
    "       ```bash\n",
    "       gunzip analyziz.tar.gz\n",
    "       ```\n",
    "    4. **Load the docker image**.  \n",
    "       ```bash\n",
    "       docker load -i analysis.tar\n",
    "       ```\n",
    "3. **Use the `docker run` command to start the docker image**. This command should be structured as follows with the exception that text in angle-brackets (`<text>`), including the brackets, should be replaced with an appropriate substitution for the local machine on which you are running the virtual machine.  \n",
    "   ```bash\n",
    "   docker run --rm -it \\\n",
    "       -p 8888:8888 \\\n",
    "       -v <HCP-restricted-data>:/data/hcp/meta/RESTRICTED_full.csv \\\n",
    "       -v \"${HOME}/.aws:/home/jovyan/.aws\" \\\n",
    "       nben/visual_autolabel:benson2025 jupyter\n",
    "   ```\n",
    "\n",
    "The above command will start a Jupyter server inside the virtual machine and will expose it on port 8888, allowing you to point a browser to `http://127.0.0.1:8888/` to connect to the virtual machine's compute environment. This environment should remain identical to that used to analyze the data for the paper. The parameter `<HCP-restricted-data>` in the command is the path on your local environment to the restricted data CSV file from the Human Connectome Project (see instruction 1C, above).\n",
    "\n",
    "You can optionally include additional volume mount data in order to save cached data across uses of the docker image or to reduce compute time. If, for example, you have the HCP subject data from the 1200 subject release loaded in the directory `/hcp/subjects` on your local computer, you can add the line `-v /hcp/subjects:/data/hcp/subjects` after the `-p 8888:8888` line. If you wish to save cache files across runs so that subsequent computations are faster, you can use put the line `-v <cache-dir>:/data` after the `-p 8888:8888` line where `<cache-dir>` should be replaced with a directory that will hold all the various cache data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65feb678-9bef-42bd-b27d-aa60a42b167a",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0c6e9-8da3-494e-9982-1ca06dc6ae17",
   "metadata": {},
   "source": [
    "Here we define any configuration item that needs to be set locally for the system running this notebook. If you are running this notebook from within the docker-iage that was published with by Benson, Song, et al. (2025) with their project (accessible [here](https://doi.org/10.5281/zenodo.14502583)), i.e. by following the instructions above, then these configuration items will not need to be changed.\n",
    "\n",
    "If you are running this notebook from outside of the docker image, then you will most likely need to edit these in order for the code to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef14da-59d9-45fd-a807-6fe17036454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_root\n",
    "# The root of the data directory, in which this notebook expects by default to\n",
    "# find the datasets, analysis, and models directories. This directory is\n",
    "# typically mounted into the docker container using the `-v` (volumes) option\n",
    "# to the docker run command.\n",
    "data_root = '/data/visual-autolabel'\n",
    "\n",
    "# dataset_cache_path\n",
    "# The directory into which data for the model training should be cached. This\n",
    "# can be None, but if it is, then the training images will need to be\n",
    "# regenerated every time the notebook is run.\n",
    "dataset_cache_path = f'{data_root}/datasets'\n",
    "\n",
    "# analysis_cache_path\n",
    "# The directory where analysis data (primarily dataframes that compare the\n",
    "# predictions of various methods) are stored.\n",
    "analysis_path = f'{data_root}/analysis'\n",
    "\n",
    "# model_cache_path\n",
    "# The directory into which to store models that are generated during training.\n",
    "# This may be None, but if it is, then the best models will not be saved out to\n",
    "# disk during rounds of training.\n",
    "model_cache_path = f'{data_root}/models'\n",
    "\n",
    "# figures_path\n",
    "# Where this notebook should save figures out to.\n",
    "figures_path = f'{data_root}/figures'\n",
    "\n",
    "# grid_path\n",
    "# Where the grid-search data either lives or should be downloaded and extracted\n",
    "# to. Note that desipte the relatively large number of files in the grid-search\n",
    "# data, the data themselves are fairly small (megabytes, not gigabytes).\n",
    "# The directory must contain another directory named 'grid-search', otherwise\n",
    "# the grid-search.tar.gz file will be extracted into the grid_path to create\n",
    "# this directory.\n",
    "grid_path = f'{data_root}/grid'\n",
    "\n",
    "# dwi_filename_pattern\n",
    "# Where and how to load diffusion-weighted imaging data files. This is only\n",
    "# important if you are regenerating the endpoint tract images from scratch;\n",
    "# otherwise this can be ignored.\n",
    "# This may be either a string or a tuple of strings; in either case all strings\n",
    "# are formatted with the target data ('rater' and 'subject') and 'hemisphere'\n",
    "# and 'tract_name' values then joined using `os.path.join`.\n",
    "# How this pattern is interpreted can be changed by editing the code for the\n",
    "# DWIFeature class below.\n",
    "dwi_filename_pattern = (\n",
    "    # We load from the directory /data/hcp/tracts/<subject_id>\n",
    "    data_root, 'tracts', '{subject}',\n",
    "    # The filename is like lh.VOF_normalized.mgz\n",
    "    '{hemisphere}.{tract_name}_normalized.mgz')\n",
    "\n",
    "# hcp_restricted_path\n",
    "# The path of the CSV file containing the HCP restricted dataset. This file\n",
    "# must include the genetic data from the HCP young adult dataset. If you\n",
    "# have configured neuropythy to have access to this file, then you may leave\n",
    "# it as None. See this website for more information: \n",
    "# https://www.humanconnectome.org/study/hcp-young-adult/document/restricted-data-usage\n",
    "hcp_restricted_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772debe5-3db9-4286-a417-4510ff88f0c7",
   "metadata": {},
   "source": [
    "## 2. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b57dad-64fb-4817-b032-8a7b8057f719",
   "metadata": {},
   "source": [
    "This section contains initialization code that loads libraries and data relevant to the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430599e-3cb7-41a3-9a50-9e4669f3ace3",
   "metadata": {},
   "source": [
    "### 2.1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d31075-26e4-4320-9810-6e868badc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import neuropythy as ny\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import visual_autolabel as va\n",
    "import visual_autolabel.benson2025 as proj\n",
    "\n",
    "proj.config.model_cache_path     = model_cache_path\n",
    "proj.config.dataset_cache_path   = dataset_cache_path\n",
    "proj.config.analysis_path        = analysis_path\n",
    "proj.config.dwi_filename_pattern = dwi_filename_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51cb04-27a5-41ab-a987-f18ef7ef3ce0",
   "metadata": {},
   "source": [
    "### 2.2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1b089-9f89-4fad-a40a-c5aea8e06d83",
   "metadata": {},
   "source": [
    "In this section we load the various data that we plan to plot in the sections below. This includes the HCP and NYU datasets, both of which are lazily loaded as subject data or model results are requested from the data structures in this section. Additionally, the grid-search data is loaded here for visualization of the hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d32ba-b055-46e0-a2ff-620507f335df",
   "metadata": {},
   "source": [
    "#### 2.2.1. HCP and NYU Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c0fba-6db2-4b48-a880-64f34ab7ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we declare the hcp datasets we are using in this notebook.\n",
    "\n",
    "# Make the HCP partition; if this fails, you probably haven't provided\n",
    "# the hcp_restricted_path correctly.\n",
    "(hcp_trn_sids, hcp_val_sids) = proj.hcp.partition(\n",
    "    hcp_restricted_path=hcp_restricted_path)\n",
    "# Make the HCP datasets for each input and output type.\n",
    "hcp_data = proj.hcp.all_datasets()\n",
    "# From these we can also make subject flatmaps:\n",
    "hcp_maps = proj.hcp.all_flatmaps(hcp_data)\n",
    "\n",
    "# Do the same for the NYU datasets.\n",
    "(nyu_trn_sids, nyu_val_sids) = proj.nyu.partition()\n",
    "nyu_data = proj.nyu.all_datasets()\n",
    "nyu_maps = proj.nyu.all_flatmaps(nyu_data)\n",
    "\n",
    "# We also want to read in the dice dataframes (which contain the dice score\n",
    "# comparisons of all models/predictors versus all others for all subjects).\n",
    "dice = proj.all_scores()\n",
    "\n",
    "# We can add a column that specifies whether the subject is in the validation or\n",
    "# the training partition; we have to add it here because the twin-status is\n",
    "# restricted data, and specifying which subjects are trn and which are val\n",
    "# provides some limited clues about which subjects are twins.\n",
    "all_val_sids = np.union1d(hcp_val_sids.astype(str), nyu_val_sids)\n",
    "is_val = np.isin(dice['sid'].astype(str), all_val_sids)\n",
    "dice.insert(1, 'partition', np.select([is_val], ['val'], 'trn'))\n",
    "# We can separate these out also:\n",
    "trndice = dice[dice['partition'] == 'trn']\n",
    "valdice = dice[dice['partition'] == 'val']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09508ca-1ab1-4c8d-ac79-54f760b33e22",
   "metadata": {},
   "source": [
    "#### 2.2.2. Hyperparameter Grid-search Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da1ae0-a7a8-40bc-9710-3952c5a5e7a1",
   "metadata": {},
   "source": [
    "The hyperparameter search results are stored in a gzipped tarball file in the [OSF repository](https://osf.io/c49dv) associated with this notebook. This code cell downloads those data and extracts them if the `'grid-search'` subdirectory isn't found in the directory named by the `grid_path` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16cb512-2f20-43cd-af9b-64480fde223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_path = Path(f\"{grid_path}/grid-search\")\n",
    "if gridsearch_path.is_dir():\n",
    "    print(f\"Grid search directory found: {gridsearch_path}\")\n",
    "else:\n",
    "    import urllib.request, tarfile\n",
    "    print(f\"Downloading and extracting grid-search.tar.gz to {grid_path}... \", end=\"\")\n",
    "    # This is the permanent URL for the grid-search.tar.gz file on the OSF:\n",
    "    url = 'https://osf.io/download/9wfde/'\n",
    "    gridsearch_filename = f'{grid_path}/grid-search.tar.gz'\n",
    "    ny.util.url_download(url, gridsearch_filename)\n",
    "    # Extract the tarball:\n",
    "    with tarfile.open(gridsearch_filename, \"r:gz\") as tar:\n",
    "        tar.extractall(str(gridsearch_path), filter=tarfile.fully_trusted_filter)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ebbfc-051c-489b-98c8-a1ff89d57c98",
   "metadata": {},
   "source": [
    "## 3. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407cd0a7-ebf5-48a9-be96-3e8227997d44",
   "metadata": {},
   "source": [
    "### 3.1. Prediction Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79000235-bad3-4ed2-a949-e5a96f697ec2",
   "metadata": {},
   "source": [
    "In this section of the analysis notebook, we plot visualizations of the accuracies of the CNN predictions as well as plots of the predictions on cortex. This part of the notebook as well as later parts use a set of 4-character codes to represent different possible input data given to the CNNs. Each CNN was trained to make predictions of either visual area boundaries (`'area'` predictions) or iso-eccentric regions (`'ring'` predictions) based on one of the following sets of input data:\n",
    " * **`'anat'`**: T1-weighted (anatomical) data only (`'x'`, `'y'`, `'z'`, `'curvature'`, `'convexity'`, `'thickness'`, `'surface_area'`).\n",
    " * **`'t1t2'`**: T1-weighted and T2-weighted data (everything from **`'anat'`** plus `'myelin'`).\n",
    " * **`'trac'`**: T1-weighted and diffusion-weighted data (everything from **`'anat'`** plus `'dwi_OR'` and `'dwi_VOF'`).\n",
    " * **`'nofn'`**: T1-weighted, T2-weighted, and diffusion-weighted data (everything from **`'t1t2'`** and from **`'trac'`**).\n",
    " * **`'func'`**: T1-weighted and functional data (everything from **`'anat'`** plus `'prf_x'`, `'prf_y'`, `'prf_sigma'`, and `'prf_cod'`).\n",
    " * **`'nodw'`**: T1-weighted, T2-weighted, and functional data (everything frmo **`'func'`** plus `'myelin'`).\n",
    " * **`'not2'`**: T1-weighted, diffusion-weighted, and functional data (everything frmo **`'func'`** plus `'dwi_OR'` and `'dwi_VOF'`).\n",
    " * **`'full'`**: All of the above data.\n",
    "\n",
    "Additionally, the following codes refer to non-CNN data sources:\n",
    " * **`'prior'`**: The retinotopic prior by [Benson & Winawer (2018)](https://doi.org/10.7554/eLife.40224).\n",
    " * **`'inf'`**: Bayesian inferred retinotopic maps of the HCP subjectsby [Benson & Winawer (2018)](https://doi.org/10.7554/eLife.40224).\n",
    " * **`'rely'`**: Inter-rater reliability of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc07960-85f9-49b9-94f4-594db19d53e4",
   "metadata": {},
   "source": [
    "#### 3.1.1. Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75036f3e-60b4-4c2c-85d5-00f7a2853d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates and returns a matplotlib figure of the CNN accuracies.\n",
    "# The cells below produce the figures used in the paper using this function.\n",
    "\n",
    "# Default error-bar options for the function.\n",
    "default_ebaropts = dict(middle='mean', extent='ste', fw=0.05, lw=0.5, ms=1.5)\n",
    "\n",
    "# These are the models that are plotted by the function based on the requested\n",
    "# parcellation--this variable gives the order of the columns in the figure.\n",
    "benson2025_accuracy_models = (\n",
    "    # We always plot the prior and the inferred maps as the first two columns.\n",
    "    ('prior', 'hcp'), ('inf',   'hcp'),\n",
    "    # We then plot the four CNNs that did not receive functional inputs.\n",
    "    ('anat',  'hcp'), ('t1t2',  'hcp'), ('trac',  'hcp'), ('nofn',  'hcp'),\n",
    "    # We then plot the four CNNs that did receive functional inputs.\n",
    "    ('full',  'hcp'), ('func',  'hcp'), ('nodw',  'hcp'), ('not2',  'hcp'),\n",
    "    # Finally, we plot the inter-rater reliability.\n",
    "    ('rely',  'hcp'))\n",
    "# Now process that shared list into a dict for area and ring parcellations:\n",
    "benson2025_accuracy_models = {\n",
    "    # For the visual area boundaries, we also want to show the NYU data.\n",
    "    'area': (benson2025_accuracy_models \n",
    "             + (('anat', 'nyu'), ('func', 'nyu'), ('fnyu', 'nyu'))),\n",
    "    # For the ring models, we just plot the above.\n",
    "    'ring': benson2025_accuracy_models}\n",
    "\n",
    "def benson2025_accuracy_plot(\n",
    "        parcellation,\n",
    "        dataframe=valdice,\n",
    "        colwidth=1,\n",
    "        lrspace=0.2,\n",
    "        ebarlw=0.5,\n",
    "        ebarwidth=0.15,\n",
    "        ebaropts=default_ebaropts,\n",
    "        pointms=2.5,\n",
    "        ebarclr = {'lh':'k', 'rh':'k'},\n",
    "        pointclr = {'lh': (0.2, 0.8, 0.8), 'rh': (1.0, 0.4, 0.4)},\n",
    "        printsummary=True):\n",
    "    \"\"\"Generates and returns a matplotlib figure of the accuracies of the CNNs\n",
    "    from Benson, Song, et al. (2025).\n",
    "\n",
    "    This function must be passed a parcellation, either `'area'` or `'ring'`\n",
    "    for visual area boundaries or iso-eccentric regions. The resulting figure\n",
    "    plots the results for this parcellation from the given dataframe, which\n",
    "    should typically be `valdice`. The remaining options tweak the visuals of\n",
    "    the plot.\n",
    "\n",
    "    The resulting figure should be equivalent to those produces by Benson,\n",
    "    Song, et al. (2025) if run correctly in the associated docker image.\n",
    "    \"\"\"\n",
    "    df0 = dataframe\n",
    "    yticks = np.linspace(0,1,6)\n",
    "    df0 = df0[df0['parcellation'] == parcellation]\n",
    "    rois = np.unique(df0['label'].values)\n",
    "    nrois = len(rois)\n",
    "    # The models we will be plotting depend on the parcellation type:\n",
    "    mdls = benson2025_accuracy_models[parcellation]\n",
    "    nmdls = len(mdls)\n",
    "    # We want to perform Bonferroni correction on the confidence intervals we report,\n",
    "    # so we count up the number of confidence intervals we are displaying. This is\n",
    "    # the number of models times the number of hemispheres (2).\n",
    "    bfcount = 2 * nmdls\n",
    "    # Make the figure itself:\n",
    "    (fig,axs) = plt.subplots(nrois, 1, figsize=(5,nrois*1.5), dpi=72*8)\n",
    "    fig.subplots_adjust(0,0,1,1,0.1,0.15)\n",
    "    # Go through each axis/ROI area first:\n",
    "    for (roi,ax) in zip(rois, axs):\n",
    "        df1 = df0[df0['label'] == roi]\n",
    "        # We don't use spines.\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)\n",
    "        # We do use ticks.\n",
    "        ax.set_ylim([-0.025, 1.025])\n",
    "        ax.set_yticks(yticks)\n",
    "        # We draw our own y-axis vertical bar.\n",
    "        ax.set_xlim([-0.25, 0.25 + (nmdls-1)*colwidth])\n",
    "        ax.plot([-0.25,-0.25], [0,1], 'k-', zorder=-99)\n",
    "        # Plot some mesh lines.\n",
    "        for y in yticks:\n",
    "            ax.plot([-0.25, 0.25 + (nmdls-1)*colwidth], [y, y], '-',\n",
    "                    lw=0.3, c='0.65', zorder=-100)\n",
    "        for y in np.mean([yticks[:-1], yticks[1:]], 0):\n",
    "            ax.plot([-0.25, 0.25 + (nmdls-1)*colwidth], [y, y], '-',\n",
    "                    lw=0.2, c='0.85', zorder=-100)\n",
    "        # Specify the labels and xticks.\n",
    "        if roi == 'mean':\n",
    "            ax.set_ylabel(\"Mean Dice Score\")\n",
    "        elif parcellation == 'area':\n",
    "            ax.set_ylabel(f'V{roi} Dice Score')\n",
    "        else:\n",
    "            ax.set_ylabel(f'E{roi} Dice Score', weight='bold')\n",
    "        ax.set_xlabel(None)\n",
    "        ax.set_xticks([])\n",
    "        # Plot the lh and rh data side-by-side.\n",
    "        for (h,dx) in zip(['lh','rh'], [-lrspace/2, lrspace/2]):\n",
    "            df2 = df1[df1['hemisphere'] == h]\n",
    "            # We want to go through each model in the dataframe.\n",
    "            for (mdl,x) in zip(mdls, np.arange(nmdls)*colwidth + dx):\n",
    "                (tag,ds) = mdl\n",
    "                df3 = df2[(df2['tag'] == tag)]\n",
    "                df3 = df3[(df3['dataset'] == ds.upper())]\n",
    "                if len(df3) == 0:\n",
    "                    continue\n",
    "                ys = df3['score'].values\n",
    "                if roi == 'mean':\n",
    "                    (mn,mu,mx) = va.plot.summarize_dist(ys, extent='ste', bfcount=bfcount)\n",
    "                    assert np.abs((mx-mu) - (mu-mn)) < 0.00001\n",
    "                    if printsummary:\n",
    "                        print(f\"{roi} {h} {tag:5s}: {mu:5.3f} ± {mx-mu:5.3f}\")\n",
    "                va.plot.plot_distbars(x, ys, axes=ax,\n",
    "                                      lc=ebarclr[h], mc=ebarclr[h], zorder=2,\n",
    "                                      bfcount=bfcount,\n",
    "                                      **ebaropts)\n",
    "                ax.plot([x]*len(ys), ys, '.', ms=pointms, c=pointclr[h],\n",
    "                        alpha=0.05, zorder=1)\n",
    "    # And return the figure.\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86f55f-497d-473e-9723-77154fd2b96a",
   "metadata": {},
   "source": [
    "#### 3.1.2. Accuracies of the CNNs Predicting Visual Area Boundaries (**Figs. 3** & **S3**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11446d-f0eb-43a2-938d-7715bfa3948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation = 'area'\n",
    "printsummary = True  # To print the precise means ± SEMs.\n",
    "fig = benson2025_accuracy_plot(parcellation, printsummary=printsummary)\n",
    "plt.savefig(f'{data_root}/figures/summary_{parcellation}.pdf',\n",
    "            bbox_inches='tight',\n",
    "            transparent=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f63a99-80bd-4657-a962-15aa3a0c041f",
   "metadata": {},
   "source": [
    "#### 3.1.3. Accuracies of the CNNs Predicting Iso-Eccentric Regions (**Figs. 4** & **S4**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebfeec-ee14-4baf-aa30-bed0d291410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation = 'ring'\n",
    "printsummary = True  # To print the precise means ± SEMs.\n",
    "fig = benson2025_accuracy_plot(parcellation, printsummary=printsummary)\n",
    "plt.savefig(f'{data_root}/figures/summary_{parcellation}.pdf',\n",
    "            bbox_inches='tight',\n",
    "            transparent=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f83c7-920f-463c-a086-9f366ac3681e",
   "metadata": {},
   "source": [
    "### 3.2. Predictions on Cortex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ee862-1696-4b50-9221-8046a270d353",
   "metadata": {},
   "source": [
    "#### 3.2.1 Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ba901-4d39-4979-bf6d-e3d05e69a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Default options for individual cortex-plot calls used by the plot function.\n",
    "default_cortex_plot_opts = dict(\n",
    "    mask=('prf_variance_explained', 0.1, 1))\n",
    "\n",
    "def cortex_prediction_plot(\n",
    "        prefix1,\n",
    "        prefix2,\n",
    "        parcellation,\n",
    "        sid=115017,\n",
    "        cortex_plot_opts=default_cortex_plot_opts):\n",
    "    # Make the figure:\n",
    "    dpi = 72*8\n",
    "    (fig,axs) = plt.subplots(1,2, figsize=(5, 2.5), dpi=dpi)\n",
    "    fig.subplots_adjust(0,0,1,1,0,0)\n",
    "    # Make/get the flatmaps for the subject:\n",
    "    if isinstance(sid, int):\n",
    "        fmaps = hcp_maps[sid]\n",
    "    else:\n",
    "        fmaps = nyu_maps[sid]\n",
    "    # Plot each hemisphere\n",
    "    for (ax,fmap) in zip(axs, fmaps):\n",
    "        if 'color' not in cortex_plot_opts:\n",
    "            if parcellation == 'area':\n",
    "                clr = 'prf_polar_angle'\n",
    "            else:\n",
    "                clr = 'prf_eccentricity'\n",
    "            opts = dict(cortex_plot_opts, color=clr)\n",
    "        else:\n",
    "            opts = cortex_plot_opts\n",
    "        ny.cortex_plot(fmap, axes=ax, **opts)\n",
    "        # Add lines.\n",
    "        for (pre,clr) in zip([prefix1, prefix2], ['w', 'k']):\n",
    "            if pre == 'tmpl':\n",
    "                pre = 'prior'\n",
    "            elif pre == 'warp':\n",
    "                pre = 'inf'\n",
    "            pnm = f'{pre}_visual_{parcellation}'\n",
    "            p = fmap.prop(pnm)\n",
    "            (u,v) = fmap.tess.indexed_edges\n",
    "            ii = p[u] != p[v]\n",
    "            xy = fmap.coordinates\n",
    "            xy = np.mean([xy[:,u[ii]], xy[:,v[ii]]], axis=0)\n",
    "            ax.scatter(xy[0],xy[1], c=clr, s=0.5)\n",
    "        # Turn off axes.\n",
    "        ax.axis('off')\n",
    "    # Return the figure.\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0bd93a-4e6e-4faa-afbe-0ea0ef9319d8",
   "metadata": {},
   "source": [
    "#### 3.2.2 Visual Area Boundaries on Cortex (**Fig. 3**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b424ca-2948-4962-8c01-6ab7e236231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = 115017\n",
    "prefix1 = 'A1'\n",
    "prefix2 = 'anat'\n",
    "parc = 'area'\n",
    "dpi = 288\n",
    "\n",
    "fig = cortex_prediction_plot(prefix1, prefix2, parc, sid=sid)\n",
    "\n",
    "flnm = f'sample-contours_{sid}_{parc}_{prefix1}-{prefix2}.png'\n",
    "plt.savefig(\n",
    "    f'{data_root}/figures/{flnm}',\n",
    "    bbox_inches='tight',\n",
    "    dpi=dpi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06313bb0-6f29-4a2c-9f12-7fd3a533b16e",
   "metadata": {},
   "source": [
    "#### 3.2.3 Iso-Eccentric Regions on Cortex (**Fig. 4**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf52b2-a630-4806-9fbb-be82a3e0284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = 115017\n",
    "prefix1 = 'A1'\n",
    "prefix2 = 'anat'\n",
    "parc = 'ring'\n",
    "dpi = 288\n",
    "\n",
    "fig = cortex_prediction_plot(prefix1, prefix2, parc, sid=sid)\n",
    "\n",
    "flnm = f'sample-contours_{sid}_{parc}_{prefix1}-{prefix2}.png'\n",
    "plt.savefig(\n",
    "    f'{data_root}/figures/{flnm}',\n",
    "    bbox_inches='tight',\n",
    "    dpi=dpi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c7ac9-7ee6-4146-b906-918c664bc7a1",
   "metadata": {},
   "source": [
    "### 3.3. Hyperparameter Search (**Figs. S1** & **S2**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330703bf-bf0c-4e4e-bd11-e3624ca37f47",
   "metadata": {},
   "source": [
    "This section contains code for plotting and examining the results of the hyperparamter search. The hyperparameter grid-search results are stored on the OSF website under the directory `hyperparameters` in a gzipped tarball file named `grid-search.tar.gz`. This file contains one directory for each cell of the grid named `f'grid{index:05d}'` where `index` is the 0-based index of the flattened grid (e.g., `grid00000` is the directory of the first cell in the grid and `grid00001` is the directory of the second cell, etc.). Inside of each of these directories are three files: `opts.json`, `plan.json`, and `run.log`. The `opts.json` file contains the options used in all stages of the fitting, the `plan.json` file contains the specific option settings for each epoch, and the `run.log` file contains a log of the accuracies achieved during the training.\n",
    "\n",
    "The overall accuracy of the model can be found in the `run.log` file, where it appears as the lowest loss among the values in the validation dice-loss column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d678830-98f1-4111-8390-0268719598cb",
   "metadata": {},
   "source": [
    "#### 3.3.1. Loading and Plotting Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c080e-dee4-4431-92bf-11ef12086f88",
   "metadata": {},
   "source": [
    "This function (`load_cell`) loads a single cell from the grid-search; the function following it loads all cells into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d45e6-c065-4036-bcd6-a24ee0b0d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cell(cell_path): \n",
    "    \"\"\"Loads the meta-data for one cell of the grid.\n",
    "    \n",
    "    `load_cell(path)` loads the cell at the given `path`, which should be a\n",
    "    directory containing the `opts.json`, `plan.json`, and `run.log` files.\n",
    "    \"\"\"\n",
    "    cell_path = Path(cell_path)\n",
    "    with (cell_path / 'opts.json').open('rt') as fl:\n",
    "        opts = json.load(fl)\n",
    "    opts = dict({'cell_id': int(opts['model_key'][4:])}, **opts)\n",
    "    with (cell_path / 'plan.json').open('rt') as fl:\n",
    "        plan = json.load(fl)\n",
    "    for (ii,p) in enumerate(plan):\n",
    "        for (k,v) in p.items():\n",
    "            opts[f'{k}_{ii}'] = v\n",
    "    with (cell_path / 'run.log').open('rt') as fl:\n",
    "        log = fl.readlines()\n",
    "    it0s = [ii for (ii,ln) in enumerate(log) if ln.startswith('Iteration')]\n",
    "    it0s.append(-1)\n",
    "    losses = np.array(\n",
    "        [min([float(ln.split()[9])\n",
    "              for ln in log[ii0:ii1]\n",
    "              if ln.endswith('*\\n')])\n",
    "         for (ii0,ii1) in zip(it0s[:-1], it0s[1:])])\n",
    "    if len(losses) > 0:\n",
    "        opts['loss_mean'] = np.mean(losses)\n",
    "        opts['loss_median'] = np.median(losses)\n",
    "        opts['loss_std'] = np.std(losses)\n",
    "        opts['loss_max'] = np.max(losses)\n",
    "        opts['loss_min'] = np.min(losses)\n",
    "    else:\n",
    "        opts['loss_mean'] = np.nan\n",
    "        opts['loss_median'] = np.nan\n",
    "        opts['loss_std'] = np.nan\n",
    "        opts['loss_max'] = np.nan\n",
    "        opts['loss_min'] = np.nan\n",
    "    opts['partition'] = va.partition_id(opts['partition'])\n",
    "    return opts\n",
    "def load_gridsearch(gridsearch_path=gridsearch_path):\n",
    "    \"\"\"Loads all cells from the gridsearch data.\n",
    "    \n",
    "    This function requires that the grid-search.tar.gz file (available from the OSF repository\n",
    "    associated with this notebook: osf.io/c49dv) be downloaded and available in the provided\n",
    "    `gridsearch_path`. The `gridsearch_path` argument must be path of the extracted\n",
    "    `grid-search` directory.\n",
    "    \"\"\"\n",
    "    cells = [\n",
    "        load_cell(gridsearch_path / f'grid{k:05d}')\n",
    "        for k in range(1800)]  # There are 1800 cells in the grid.\n",
    "    cells_full = pd.DataFrame(cells)\n",
    "    cells = cells_full.drop(\n",
    "        columns=[\n",
    "            'pretrained', 'multiproc', \n",
    "            'model_cache_path', 'data_cache_path',\n",
    "            'model_key', 'partition',\n",
    "            'lr_1', 'lr_2', 'bce_weight_1', 'bce_weight_2'])\n",
    "    return cells.rename(columns=dict(lr_0='lr', bce_weight_0='bce_weight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9efc7-740a-4c46-a663-3c525323efc0",
   "metadata": {},
   "source": [
    "This cell contains visualization code, i.e. to plot the grids based on particular conditions. The plots are made as a grid of grids where the small grids plot `lr` versus `gamma` and the outer grids capture the remaining hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5dfc82-039b-4c2b-a128-e2ef83e76769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotgrid(cells, inputs, outputs, base_model, col='loss_min',\n",
    "             vmin=0, vmax=0.2, cmap='cividis',\n",
    "             figsize=(7,7), dpi=512,\n",
    "             gammas=(0.8, 0.85, 0.9, 0.95, 1),\n",
    "             lrs=(0.00167, 0.0025 , 0.00375, 0.00562, 0.00844),\n",
    "             axes=None,\n",
    "             star=True,\n",
    "             printbest=False):\n",
    "    from warnings import warn\n",
    "    # First get the set of cells we are planning to use:\n",
    "    cells = cells[(cells['inputs'] == inputs) & (cells['prediction'] == outputs)]\n",
    "    cells = cells[cells['base_model'] == base_model]\n",
    "    # We now need a grid of 3 x 3 matrices, each of which will be 5x5:\n",
    "    if axes is None:\n",
    "        (fig,axs) = plt.subplots(3, 3, dpi=dpi, figsize=figsize, sharex=True, sharey=True)\n",
    "    else:\n",
    "        axs = axes\n",
    "        fig = None\n",
    "    posmin = None\n",
    "    totmin = np.inf\n",
    "    mtcs = []\n",
    "    for (axrow,batch_size) in zip(axs, [2,4,6]):\n",
    "        subcells0 = cells[cells['batch_size'] == batch_size]\n",
    "        mtxrow = []\n",
    "        for (ax, bcew0) in zip(axrow, [0.5, 0.67, 0.75]):\n",
    "            # Get the subset of cells that match:\n",
    "            subcells = subcells0[subcells0['bce_weight'] == bcew0]\n",
    "            # Now go through and make the matrix (there may be values missing, so we\n",
    "            # build this up iteratively).\n",
    "            mtx = []\n",
    "            g = []\n",
    "            l = []\n",
    "            for gamma in gammas:\n",
    "                row = []\n",
    "                for lr in lrs:\n",
    "                    g.append(gamma)\n",
    "                    l.append(lr)\n",
    "                    cell = subcells[(subcells['gamma'] == gamma) & (subcells['lr'] == lr)]\n",
    "                    if len(cell) == 0:\n",
    "                        warn(f\"missing cell: {inputs}/{outputs}/\"\n",
    "                             f\"{base_model}/{batch_size}/{bcew0}/{gamma}/{lr}\")\n",
    "                        row.append(np.nan)\n",
    "                    elif len(cell) > 1:\n",
    "                        warn(f\"identical cells: {[r['cell_id'] for (ii,r) in cell.iterrows()]}\")\n",
    "                        row.append(np.nan)\n",
    "                    else:\n",
    "                        row.append(cell[col].values[0])\n",
    "                mtx.append(row)\n",
    "            mtx = np.round(np.array(mtx), 3)\n",
    "            mtcs.append(mtx)\n",
    "            ax.imshow(mtx, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_title(f'batch={batch_size}, BCE$_0$={bcew0}')\n",
    "            # We want to track the smallest value\n",
    "            mtxmin_ii = np.nanargmin(mtx)\n",
    "            mtxmin = mtx.flat[mtxmin_ii]\n",
    "            if mtxmin < totmin:\n",
    "                totmin = mtxmin\n",
    "                posmin = {\n",
    "                    'batch_size': batch_size, 'bce_weight': bcew0,\n",
    "                    'gamma': g[mtxmin_ii], 'lr': l[mtxmin_ii],\n",
    "                    'value': totmin}\n",
    "    for ax in axs[:,0]:\n",
    "        ax.set_ylabel(r'$\\gamma$')\n",
    "        ax.set_yticks(range(len(gammas)))\n",
    "        ax.set_yticklabels(gammas)\n",
    "    for ax in axs[-1,:]:\n",
    "        ax.set_xlabel(r'Learning Rate [$10^{-3}$]')\n",
    "        ax.set_xticks(range(len(lrs)))\n",
    "        ax.set_xticklabels([f'{lr*1000:3.2f}' for lr in lrs])\n",
    "    if star and np.isfinite(totmin):\n",
    "        for (ax,mtx) in zip(axs.flat, mtcs):\n",
    "            for (ri,row) in enumerate(mtx):\n",
    "                for (ci,val) in enumerate(row):\n",
    "                    if np.round(val, 3) <= totmin:\n",
    "                        ax.plot(ci, ri, 'w*', zorder=10)\n",
    "                        if printbest:\n",
    "                            print(ci, ri, np.round(val, 3))\n",
    "    if fig is not None:\n",
    "        fig.subplots_adjust(0,0,1,1,0.2,0.2)\n",
    "    return posmin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d589988-1554-44d2-b6f6-892eaf9006f4",
   "metadata": {},
   "source": [
    "#### Visualization of the Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc15e3-aa32-493b-9a73-c0040ffb272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load in the grid-search dataframe.\n",
    "grid = load_gridsearch()\n",
    "\n",
    "# We might want to adjust the colormap scale for each subpart of the images; however, we might\n",
    "# alternatively want them all the same. We can adjust that here:\n",
    "kws = {('anat','area'): dict(vmin=0.05, vmax=0.4, cmap='hot'),\n",
    "       ('full','area'): dict(vmin=0.05, vmax=0.4, cmap='hot'),\n",
    "       ('anat','ring'): dict(vmin=0.05, vmax=0.4, cmap='hot'),\n",
    "       ('full','ring'): dict(vmin=0.05, vmax=0.4, cmap='hot')}\n",
    "\n",
    "# Now make the plots:\n",
    "for (inputs,outputs) in [('anat','area'),('full','area'),('anat','ring'),('full','ring')]:\n",
    "    kw = kws[(inputs, outputs)]\n",
    "    (fig,axs) = plt.subplots(3, 6, figsize=(18,9), dpi=512, sharex=True, sharey=True)\n",
    "    min18 = plotgrid(cells, inputs, outputs, 'resnet18', axes=axs[:,:3], **kw)\n",
    "    min34 = plotgrid(cells, inputs, outputs, 'resnet34', axes=axs[:,3:], **kw)\n",
    "    # Print the inputs/outputs that this plot represents plus the best result from the\n",
    "    # ResNet18 and ResNet34 for comparison.\n",
    "    print(f'[{inputs}:{outputs}]  ResNet18: {min18[\"value\"]:3.2f};  ResNet34: {min34[\"value\"]:3.2f}')\n",
    "    for ax in axs[:,3]:\n",
    "        ax.set_ylabel('')\n",
    "    fig.subplots_adjust(0,0,1,1,0.25,0.25)\n",
    "    plt.savefig(f'/data/visual-autolabel/figures/grid_{inputs}_{outputs}.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
