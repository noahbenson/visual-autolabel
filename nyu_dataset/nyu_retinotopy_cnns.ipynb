{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "import numpy as np\n",
    "import neuropythy as ny\n",
    "import pimms\n",
    "import matplotlib as mpl, matplotlib.pyplot as plt\n",
    "import ipyvolume as ipv\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from collections.abc import Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Where is the visual_autolabel library?\n",
    "visual_autolabel_path = '/scratch/bs4283/visual_autolabel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if visual_autolabel_path not in sys.path:\n",
    "    sys.path.append(visual_autolabel_path)\n",
    "import visual_autolabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyu_retinotopy_pp = ny.util.pseudo_path(\n",
    "    's3://openneuro.org/ds003787/',\n",
    "    cache_path='/scratch/bs4283/visual_autolabel/data/openneuro/ds003787')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nyu_fssubj(subj_id, path=nyu_retinotopy_pp, max_eccen=12.4):\n",
    "    # Get the FreeSurfer subject directory:\n",
    "    subpp = path.subpath(f'derivatives/freesurfer/{subj_id}')\n",
    "    sub = ny.freesurfer_subject(subpp)\n",
    "    # Get the PRF subpath:\n",
    "    prfpp = path.subpath(f'derivatives/prfanalyze-vista/{subj_id}/ses-nyu3t01/')\n",
    "    labpp = path.subpath(f'derivatives/ROIs/{subj_id}/')\n",
    "    for h in ['lh', 'rh']:\n",
    "        hem = sub.hemis[h]\n",
    "        # Load the retinotopy data for this hemisphere:\n",
    "        prfs = dict(\n",
    "            prf_polar_angle=ny.load(prfpp.local_path(f'{h}.angle_adj.mgz')),\n",
    "            prf_eccentricity=ny.load(prfpp.local_path(f'{h}.eccen.mgz')),\n",
    "            prf_variance_explained=ny.load(prfpp.local_path(f'{h}.vexpl.mgz')),\n",
    "            prf_radius=ny.load(prfpp.local_path(f'{h}.sigma.mgz')),\n",
    "            prf_x=ny.load(prfpp.local_path(f'{h}.x.mgz')),\n",
    "            prf_y=ny.load(prfpp.local_path(f'{h}.y.mgz')),\n",
    "            label=ny.load(labpp.local_path(f'{h}.ROIs_V1-4.mgz')))\n",
    "        # Add scaled eccentricity to the subject:\n",
    "        prfs['prf_scaled_eccentricity'] = prfs['prf_eccentricity'] / max_eccen * 8\n",
    "        prfs['prf_scaled_x'] = prfs['prf_eccentricity'] / max_eccen * 8\n",
    "        prfs['prf_scaled_y'] = prfs['prf_eccentricity'] / max_eccen * 8\n",
    "        hem = hem.with_prop(prfs)\n",
    "        sub = sub.with_hemi({h: hem})\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ImageCache class for the NYU Retinotopy Dataset\n",
    "\n",
    "See the `HCPImageCache` class in the `visual_autolabel.image._hcp` namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_autolabel.image import (\n",
    "    BilateralFlatmapImageCache,\n",
    "    FlatmapFeature,\n",
    "    LabelFeature\n",
    ")\n",
    "\n",
    "class NYURetinotopyImageCache(BilateralFlatmapImageCache):\n",
    "    \"\"\"An ImageCache subclass that handles features of the NYU\n",
    "    Retinotopy Dataset Occipital Pole.\n",
    "    \"\"\"\n",
    "    # The pseudo-path for the NYU retinotopy dataset:\n",
    "    nyu_retinotopy_pp = ny.util.pseudo_path(\n",
    "        's3://openneuro.org/ds003787/',\n",
    "        cache_path='/data/openneuro/ds003787')\n",
    "    # The subject list:\n",
    "    subject_list = [\n",
    "        'sub-wlsubj001',\n",
    "        'sub-wlsubj004',\n",
    "        'sub-wlsubj006',\n",
    "        'sub-wlsubj007',\n",
    "        'sub-wlsubj014',\n",
    "        'sub-wlsubj019',\n",
    "        'sub-wlsubj023',\n",
    "        'sub-wlsubj042',\n",
    "        'sub-wlsubj043',\n",
    "        'sub-wlsubj045',\n",
    "        'sub-wlsubj046',\n",
    "        'sub-wlsubj055',\n",
    "        'sub-wlsubj056',\n",
    "        'sub-wlsubj057',\n",
    "        'sub-wlsubj062',\n",
    "        'sub-wlsubj064',\n",
    "        'sub-wlsubj067',\n",
    "        'sub-wlsubj071',\n",
    "        'sub-wlsubj076',\n",
    "        'sub-wlsubj079',\n",
    "        'sub-wlsubj081',\n",
    "        'sub-wlsubj083',\n",
    "        'sub-wlsubj084',\n",
    "        'sub-wlsubj085',\n",
    "        'sub-wlsubj086',\n",
    "        'sub-wlsubj087',\n",
    "        'sub-wlsubj088',\n",
    "        'sub-wlsubj090',\n",
    "        'sub-wlsubj091',\n",
    "        'sub-wlsubj092',\n",
    "        'sub-wlsubj094',\n",
    "        'sub-wlsubj095',\n",
    "        'sub-wlsubj104',\n",
    "        'sub-wlsubj105',\n",
    "        'sub-wlsubj109',\n",
    "        'sub-wlsubj114',\n",
    "        'sub-wlsubj115',\n",
    "        'sub-wlsubj116',\n",
    "        'sub-wlsubj117',\n",
    "        'sub-wlsubj118',\n",
    "        'sub-wlsubj120',\n",
    "        'sub-wlsubj121',\n",
    "        'sub-wlsubj122',\n",
    "        'sub-wlsubj126']\n",
    "    # The NYU Retinotopy subjects:\n",
    "    subjects = pimms.lazy_map(\n",
    "        {s: ny.util.curry(lambda s: nyu_fssubj(s), s)\n",
    "         for s in subject_list})\n",
    "    # The featuers we know how to make.\n",
    "    _builtin_features = {\n",
    "        # Functional Features first.\n",
    "        'prf_polar_angle':  FlatmapFeature('prf_polar_angle', 'nearest'),\n",
    "        'prf_eccentricity': FlatmapFeature('prf_eccentricity', 'linear'),\n",
    "        'prf_cod':          FlatmapFeature('prf_variance_explained', 'linear'),\n",
    "        'prf_sigma':        FlatmapFeature('prf_radius', 'linear'),\n",
    "        'prf_x':            FlatmapFeature('prf_x', 'linear'),\n",
    "        'prf_y':            FlatmapFeature('prf_y', 'linear'),\n",
    "        'prf_scaled_eccentricity': FlatmapFeature('prf_scaled_eccentricity', 'linear'),\n",
    "        'prf_scaled_x':            FlatmapFeature('prf_scaled_x', 'linear'),\n",
    "        'prf_scaled_y':            FlatmapFeature('prf_scaled_y', 'linear'),\n",
    "        # The vertex coordinates themselves; we add these in.\n",
    "        'x': FlatmapFeature('midgray_x', 'linear'),\n",
    "        'y': FlatmapFeature('midgray_y', 'linear'),\n",
    "        'z': FlatmapFeature('midgray_z', 'linear'),\n",
    "        # The visual area and visual sector-based features.\n",
    "        \n",
    "        'V1':  LabelFeature('label:1', 'nearest'),\n",
    "        'V2':  LabelFeature('label:2', 'nearest'),\n",
    "        'V3':  LabelFeature('label:3', 'nearest')\n",
    "#        'V1':    LabelFeature('visual_area:1', 'nearest'),\n",
    "#        'V2':    LabelFeature('visual_area:2', 'nearest'),\n",
    "#        'V3':    LabelFeature('visual_area:3', 'nearest')\n",
    "    }\n",
    "    @classmethod\n",
    "    def builtin_features(cls):\n",
    "        fs = NYURetinotopyImageCache._builtin_features\n",
    "        return dict(BilateralFlatmapImageCache.builtin_features(), **fs)\n",
    "    @classmethod\n",
    "    def unpack_target(cls, target):\n",
    "        if isinstance(target, Mapping):\n",
    "            sid = target['subject']\n",
    "        else:\n",
    "            sid = target\n",
    "        return (sid,)\n",
    "    def cache_filename(self, target, feature):\n",
    "        return os.path.join(feature, f\"{target}.pt\")\n",
    "    def make_flatmap(self, target, view=None):\n",
    "        # We may have been given (rater, sid, h) or ((rater, sid), h):\n",
    "        (sid,) = self.unpack_target(target)\n",
    "        if view is None:\n",
    "            raise ValueError(\"NYURetinotopyImageCache requires a view\")\n",
    "        h = view['hemisphere']\n",
    "        # Get the subject and hemi.\n",
    "        sub = NYURetinotopyImageCache.subjects[sid]\n",
    "        hem = sub.hemis[h]\n",
    "        # Fix the properties now, if needed:\n",
    "        (x,y,z) = hem.surface('midgray').coordinates\n",
    "        hem = hem.with_prop(midgray_x=x, midgray_y=y, midgray_z=z)\n",
    "        # Make the flatmap:\n",
    "        fmap = ny.to_flatmap('occipital_pole', hem, radius=np.pi/2.25)\n",
    "        fmap = fmap.with_meta(subject_id=sid, hemisphere=h)\n",
    "        # And return!\n",
    "        return fmap\n",
    "    # We overload fill_image so that we can call down then turn NaNs into 0s.\n",
    "    def fill_image(self, target, feature, im):\n",
    "        super().fill_image(target, feature, im)\n",
    "        im[torch.isnan(im)] = 0\n",
    "        return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinotopy_cache = NYURetinotopyImageCache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. image shape changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def resize_tensor(tensor,new_shape):\n",
    "    return F.interpolate(tensor,size = new_shape,mode = 'bilinear',align_corners = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model and model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ww = torch.load('/scratch/bs4283/visual_autolabel/best_func.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def convrelu(in_channels, out_channels,\n",
    "             kernel=3, padding=None, stride=1, bias=True, inplace=True):\n",
    "    \"\"\"Shortcut for creating a PyTorch 2D convolution followed by a ReLU.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        The number of input channels in the convolution.\n",
    "    out_channels : int\n",
    "        The number of output channels in the convolution.\n",
    "    kernel : int, optional\n",
    "        The kernel size for the convolution (default: 3).\n",
    "    padding : int or None, optional\n",
    "        The padding size for the convolution; if `None` (the default), then\n",
    "        chooses a padding size that attempts to maintain the image-size.\n",
    "    stride : int, optional\n",
    "        The stride to use in the convolution (default: 1).\n",
    "    bias : boolean, optional\n",
    "        Whether the convolution has a learnable bias (default: True).\n",
    "    inplace : boolean, optional\n",
    "        Whether to perform the ReLU operation in-place (default: True).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Sequential\n",
    "        The model of a 2D-convolution followed by a ReLU operation.\n",
    "    \"\"\"\n",
    "#    if padding is None:\n",
    "#        padding = kernel_default_padding(kernel)\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels, out_channels, kernel,\n",
    "                        padding=padding, bias=bias),\n",
    "        torch.nn.ReLU(inplace=inplace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    \"\"\"a U-Net with a ResNet18 backbone for learning visual area labels.\n",
    "\n",
    "    The `UNet` class implements a [\"U-Net\"](https://arxiv.org/abs/1505.04597)\n",
    "    with a [ResNet-18](https://pytorch.org/hub/pytorch_vision_resnet/) bacbone.\n",
    "    The class inherits from `torch.nn.Module`.\n",
    "    \n",
    "    The original implementation of this class was by Shaoling Chen\n",
    "    (sc6995@nyu.edu), and additional modifications have been made by Noah C.\n",
    "    Benson (nben@uw.edu).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_count : int\n",
    "        The number of channels (features) in the input image. When using an\n",
    "        `HCPVisualDataset` object for training, this value should be set to 4\n",
    "        if the dataset uses the `'anat'` or `'func'` features and 8 if it uses\n",
    "        the `'both'` features.\n",
    "    segment_count : int\n",
    "        The number of segments (AKA classes, labels) in the output data. For\n",
    "        V1-V3 this is typically either 3 (V1, V2, V3) or 6 (LV1, LV2, LV3, RV1,\n",
    "        RV2, RV3).\n",
    "    base_model : model name or tuple, optional\n",
    "        The name of the model that is to be used as the base/backbone of the\n",
    "        UNet. The default is `'resnet18'`, but \n",
    "    pretrained : boolean, optional\n",
    "        Whether to use a pretrained base model for the backbone (`True`) or not\n",
    "        (`False`). The default is `False`.\n",
    "    logits : boolean, optional\n",
    "        Whether the model should return logits (`True`) or probabilities\n",
    "        (`False`). The default is `True`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    pretrained_base : boolean\n",
    "        `True` if the base model used in this `UNet` was originally pre-trained\n",
    "        and `False` otherwise.\n",
    "    base_model : PyTorch Module\n",
    "        The ResNet-18 model that is used as the backbone of the `UNet` model.\n",
    "    base_layers : list of PyTorch Modules\n",
    "        The ResNet-18 layers that are used in the backbone of the `UNet` model.\n",
    "    feature_count : int\n",
    "        The number of input channels (features) that the model expects in input\n",
    "        images.\n",
    "    segment_count : int\n",
    "        The number of segments (labels) predicted by the model.\n",
    "    logits : bool\n",
    "        `True` if the output of the model is in logits and `False` if its output\n",
    "        is in probabilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_count, segment_count,\n",
    "                 base_model='resnet18',\n",
    "                 pretrained=False,\n",
    "                 logits=False):\n",
    "        import torch.nn as nn\n",
    "        # Initialize the super-class.\n",
    "        super().__init__()\n",
    "        # Store some basic attributes.\n",
    "        self.feature_count = feature_count\n",
    "        self.segment_count = segment_count\n",
    "        self.pretrained = pretrained\n",
    "        self.logits = logits\n",
    "        # Set up the base model and base layers for the model.\n",
    "        if pretrained:\n",
    "            weights = 'IMAGENET1K_V1'\n",
    "        else:\n",
    "            weights = None\n",
    "        import torchvision.models as mdls\n",
    "        base_model = getattr(mdls, base_model)\n",
    "        try:\n",
    "            base_model = base_model(weights=weights,\n",
    "                                    num_classes=segment_count)\n",
    "        except TypeError:\n",
    "            base_model = base_model(pretrained=pretrained,\n",
    "                                    num_classes=segment_count)\n",
    "        # Not sure we should store the base model; seems like a good idea, but\n",
    "        # does it get caught up in PyTorch's Module data when we do?\n",
    "        #self.base_model = resnet18(pretrained=pretrained)\n",
    "        # Because the input size may not be 3 and the output size may not be 3,\n",
    "        # we want to add an additional \n",
    "        if feature_count != 3:\n",
    "            # Adjust the first convolution's number of input channels.\n",
    "            c1 = base_model.conv1\n",
    "            base_model.conv1 = nn.Conv2d(\n",
    "                feature_count, c1.out_channels,\n",
    "                kernel_size=c1.kernel_size, stride=c1.stride,\n",
    "                padding=c1.padding, bias=c1.bias)\n",
    "        base_layers = list(base_model.children())\n",
    "        #self.base_layers = base_layers\n",
    "        # Make the U-Net layers out of the base-layers.\n",
    "        # size = (N, 64, H/2, W/2)\n",
    "        self.layer0 = nn.Sequential(*base_layers[:3]) \n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        # size = (N, 64, H/4, W/4)\n",
    "        self.layer1 = nn.Sequential(*base_layers[3:5])\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        # size = (N, 128, H/8, W/8)        \n",
    "        self.layer2 = base_layers[5]\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        # size = (N, 256, H/16, W/16)\n",
    "        self.layer3 = base_layers[6]  \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        # size = (N, 512, H/32, W/32)\n",
    "        self.layer4 = base_layers[7]\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "        # The up-swing of the UNet; we will need to upsample the image.\n",
    "        self.upsample = nn.Upsample(scale_factor=2,\n",
    "                                    mode='bilinear',\n",
    "                                    align_corners=True)\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        self.conv_original_size0 = convrelu(feature_count, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        self.conv_last = nn.Conv2d(64, segment_count, 1)\n",
    "    def forward(self, input):\n",
    "        # Do the original size convolutions.\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        # Now the front few layers, which we save for adding back in on the UNet\n",
    "        # up-swing below.\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        # Now, we start the up-swing; each step must upsample the image.\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        # Up-swing Step 1\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "        # Up-swing Step 2\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "        # Up-swing Step 3\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "        # Up-swing Step 4\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        # Up-swing Step 5\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        # And the final convolution.\n",
    "        out = self.conv_last(x)\n",
    "        if not self.logits:\n",
    "            out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unet  = UNet(feature_count = 11, segment_count = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set weight parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model_unet.layer0[0].weight = nn.Parameter(model_ww['layer0.0.weight'],requires_grad=False)\n",
    "model_unet.layer0[1].weight = nn.Parameter(model_ww['layer0.1.weight'],requires_grad=False)\n",
    "model_unet.layer0[1].bias = nn.Parameter(model_ww['layer0.1.bias'],requires_grad=False)\n",
    "model_unet.layer0[1].running_mean = nn.Parameter(model_ww['layer0.1.running_mean'],requires_grad=False)\n",
    "model_unet.layer0[1].running_var = nn.Parameter(model_ww['layer0.1.running_var'],requires_grad=False)\n",
    "model_unet.layer0[1].num_batches_tracked = nn.Parameter(model_ww['layer0.1.num_batches_tracked'],requires_grad=False)\n",
    "model_unet.layer0_1x1[0].weight = nn.Parameter(model_ww['layer0_1x1.0.weight'],requires_grad=False)\n",
    "model_unet.layer0_1x1[0].bias = nn.Parameter(model_ww['layer0_1x1.0.bias'],requires_grad=False)\n",
    "model_unet.layer1[1][0].conv1.weight = nn.Parameter(model_ww['layer1.1.0.conv1.weight'],requires_grad=False)\n",
    "model_unet.layer1[1][0].bn1.weight = nn.Parameter(model_ww['layer1.1.0.bn1.weight'],requires_grad=False)\n",
    "model_unet.layer1[1][0].bn1.bias = nn.Parameter(model_ww['layer1.1.0.bn1.bias'],requires_grad=False)\n",
    "model_unet.layer1[1][0].bn1.running_mean = nn.Parameter(model_ww['layer1.1.0.bn1.running_mean'],requires_grad=False)\n",
    "model_unet.layer1[1][0].bn1.running_var = nn.Parameter(model_ww['layer1.1.0.bn1.running_var'],requires_grad=False)\n",
    "model_unet.layer1[1][0].bn1.num_batches_tracked = nn.Parameter(model_ww['layer1.1.0.bn1.num_batches_tracked'],requires_grad=False)\n",
    "model_unet.layer1[1][0].conv2.weight = nn.Parameter(model_ww['layer1.1.0.conv2.weight'],requires_grad=False)\n",
    "model_unet.layer1[1][0].bn2.weight = nn.Parameter(model_ww['layer1.1.0.bn2.weight'],requires_grad=False)\n",
    "\n",
    "\n",
    "\n",
    "model_unet.layer1[1][0].bn2.bias = nn.Parameter(model_ww['layer1.1.0.bn2.bias'],requires_grad=False)\n",
    "model_unet.layer1[1][0].bn2.running_mean = nn.Parameter(model_ww['layer1.1.0.bn2.running_mean'],requires_grad=False)\n",
    "model_unet.layer1[1][0].bn2.running_var = nn.Parameter(model_ww['layer1.1.0.bn2.running_var'],requires_grad=False)\n",
    "model_unet.layer1[1][0].bn2.num_batches_tracked = nn.Parameter(model_ww['layer1.1.0.bn2.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "\n",
    "model_unet.layer1[1][1].conv1.weight = nn.Parameter(model_ww['layer1.1.1.conv1.weight'],requires_grad=False)\n",
    "model_unet.layer1[1][1].bn1.weight = nn.Parameter(model_ww['layer1.1.1.bn1.weight'],requires_grad=False)\n",
    "model_unet.layer1[1][1].bn1.bias = nn.Parameter(model_ww['layer1.1.1.bn1.bias'],requires_grad=False)\n",
    "model_unet.layer1[1][1].bn1.running_mean = nn.Parameter(model_ww['layer1.1.1.bn1.running_mean'],requires_grad=False)\n",
    "model_unet.layer1[1][1].bn1.running_var = nn.Parameter(model_ww['layer1.1.1.bn1.running_var'],requires_grad=False)\n",
    "model_unet.layer1[1][1].bn1.num_batches_tracked = nn.Parameter(model_ww['layer1.1.1.bn1.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "\n",
    "model_unet.layer1[1][1].conv2.weight = nn.Parameter(model_ww['layer1.1.1.conv2.weight'],requires_grad=False)\n",
    "model_unet.layer1[1][1].bn2.weight = nn.Parameter(model_ww['layer1.1.1.bn2.weight'],requires_grad=False)\n",
    "model_unet.layer1[1][1].bn2.bias = nn.Parameter(model_ww['layer1.1.1.bn2.bias'],requires_grad=False)\n",
    "model_unet.layer1[1][1].bn2.running_mean = nn.Parameter(model_ww['layer1.1.1.bn2.running_mean'],requires_grad=False)\n",
    "model_unet.layer1[1][1].bn2.running_var = nn.Parameter(model_ww['layer1.1.1.bn2.running_var'],requires_grad=False)\n",
    "model_unet.layer1[1][1].bn2.num_batches_tracked = nn.Parameter(model_ww['layer1.1.1.bn2.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "model_unet.layer1_1x1[0].weight = nn.Parameter(model_ww['layer1_1x1.0.weight'],requires_grad=False)\n",
    "model_unet.layer1_1x1[0].bias = nn.Parameter(model_ww['layer1_1x1.0.bias'],requires_grad=False)\n",
    "\n",
    "model_unet.layer2[0].conv1.weight = nn.Parameter(model_ww['layer2.0.conv1.weight'],requires_grad=False)\n",
    "model_unet.layer2[0].bn1.weight = nn.Parameter(model_ww['layer2.0.bn1.weight'],requires_grad=False)\n",
    "model_unet.layer2[0].bn1.bias = nn.Parameter(model_ww['layer2.0.bn1.bias'],requires_grad=False)\n",
    "model_unet.layer2[0].bn1.running_mean = nn.Parameter(model_ww['layer2.0.bn1.running_mean'],requires_grad=False)\n",
    "model_unet.layer2[0].bn1.running_var = nn.Parameter(model_ww['layer2.0.bn1.running_var'],requires_grad=False)\n",
    "model_unet.layer2[0].bn1.num_batches_tracked = nn.Parameter(model_ww['layer2.0.bn1.num_batches_tracked'],requires_grad=False)\n",
    "model_unet.layer2[0].conv2.weight = nn.Parameter(model_ww['layer2.0.conv2.weight'],requires_grad=False)\n",
    "model_unet.layer2[0].bn2.weight = nn.Parameter(model_ww['layer2.0.bn2.weight'],requires_grad=False)\n",
    "model_unet.layer2[0].bn2.bias = nn.Parameter(model_ww['layer2.0.bn2.bias'],requires_grad=False)\n",
    "model_unet.layer2[0].bn2.running_mean = nn.Parameter(model_ww['layer2.0.bn2.running_mean'],requires_grad=False)\n",
    "model_unet.layer2[0].bn2.running_var = nn.Parameter(model_ww['layer2.0.bn2.running_var'],requires_grad=False)\n",
    "model_unet.layer2[0].bn2.num_batches_tracked = nn.Parameter(model_ww['layer2.0.bn2.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "\n",
    "model_unet.layer2[0].downsample[0].weight = nn.Parameter(model_ww['layer2.0.downsample.0.weight'],requires_grad=False)\n",
    "model_unet.layer2[0].downsample[1].weight = nn.Parameter(model_ww['layer2.0.downsample.1.weight'],requires_grad=False)\n",
    "model_unet.layer2[0].downsample[1].bias = nn.Parameter(model_ww[ 'layer2.0.downsample.1.bias'],requires_grad=False)\n",
    "model_unet.layer2[0].downsample[1].running_mean = nn.Parameter(model_ww['layer2.0.downsample.1.running_mean'],requires_grad=False)\n",
    "model_unet.layer2[0].downsample[1].running_var = nn.Parameter(model_ww['layer2.0.downsample.1.running_var'],requires_grad=False)\n",
    "model_unet.layer2[0].downsample[1].num_batches_tracked = nn.Parameter(model_ww['layer2.0.downsample.1.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "\n",
    "model_unet.layer2[1].conv1.weight = nn.Parameter(model_ww['layer2.1.conv1.weight'],requires_grad=False)\n",
    "model_unet.layer2[1].bn1.weight = nn.Parameter(model_ww['layer2.1.bn1.weight'],requires_grad=False)\n",
    "model_unet.layer2[1].bn1.bias = nn.Parameter(model_ww['layer2.1.bn1.bias'],requires_grad=False)\n",
    "model_unet.layer2[1].bn1.running_mean = nn.Parameter(model_ww['layer2.1.bn1.running_mean'],requires_grad=False)\n",
    "model_unet.layer2[1].bn1.running_var = nn.Parameter(model_ww['layer2.1.bn1.running_var'],requires_grad=False)\n",
    "model_unet.layer2[1].bn1.num_batches_tracked = nn.Parameter(model_ww['layer2.1.bn1.num_batches_tracked'],requires_grad=False)\n",
    "model_unet.layer2[1].conv2.weight = nn.Parameter(model_ww['layer2.1.conv2.weight'],requires_grad=False)\n",
    "model_unet.layer2[1].bn2.weight = nn.Parameter(model_ww['layer2.1.bn2.weight'],requires_grad=False)\n",
    "model_unet.layer2[1].bn2.bias = nn.Parameter(model_ww['layer2.1.bn2.bias'],requires_grad=False)\n",
    "model_unet.layer2[1].bn2.running_mean = nn.Parameter(model_ww['layer2.1.bn2.running_mean'],requires_grad=False)\n",
    "model_unet.layer2[1].bn2.running_var = nn.Parameter(model_ww['layer2.1.bn2.running_var'],requires_grad=False)\n",
    "model_unet.layer2[1].bn2.num_batches_tracked = nn.Parameter(model_ww['layer2.1.bn2.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "model_unet.layer2_1x1[0].weight = nn.Parameter(model_ww['layer2_1x1.0.weight'],requires_grad=False)\n",
    "model_unet.layer2_1x1[0].bias = nn.Parameter(model_ww['layer2_1x1.0.bias'],requires_grad=False)\n",
    "\n",
    "\n",
    "model_unet.layer3[0].conv1.weight = nn.Parameter(model_ww['layer3.0.conv1.weight'],requires_grad=False)\n",
    "model_unet.layer3[0].bn1.weight = nn.Parameter(model_ww['layer3.0.bn1.weight'],requires_grad=False)\n",
    "model_unet.layer3[0].bn1.bias = nn.Parameter(model_ww['layer3.0.bn1.bias'],requires_grad=False)\n",
    "model_unet.layer3[0].bn1.running_mean = nn.Parameter(model_ww['layer3.0.bn1.running_mean'],requires_grad=False)\n",
    "model_unet.layer3[0].bn1.running_var = nn.Parameter(model_ww['layer3.0.bn1.running_var'],requires_grad=False)\n",
    "model_unet.layer3[0].bn1.num_batches_tracked = nn.Parameter(model_ww['layer3.0.bn1.num_batches_tracked'],requires_grad=False)\n",
    "model_unet.layer3[0].conv2.weight = nn.Parameter(model_ww['layer3.0.conv2.weight'],requires_grad=False)\n",
    "model_unet.layer3[0].bn2.weight = nn.Parameter(model_ww['layer3.0.bn2.weight'],requires_grad=False)\n",
    "model_unet.layer3[0].bn2.bias = nn.Parameter(model_ww['layer3.0.bn2.bias'],requires_grad=False)\n",
    "model_unet.layer3[0].bn2.running_mean = nn.Parameter(model_ww['layer3.0.bn2.running_mean'],requires_grad=False)\n",
    "model_unet.layer3[0].bn2.running_var = nn.Parameter(model_ww['layer3.0.bn2.running_var'],requires_grad=False)\n",
    "model_unet.layer3[0].bn2.num_batches_tracked = nn.Parameter(model_ww['layer3.0.bn2.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "model_unet.layer3[0].downsample[0].weight = nn.Parameter(model_ww['layer3.0.downsample.0.weight'],requires_grad=False)\n",
    "model_unet.layer3[0].downsample[1].weight = nn.Parameter(model_ww['layer3.0.downsample.1.weight'],requires_grad=False)\n",
    "model_unet.layer3[0].downsample[1].bias = nn.Parameter(model_ww[ 'layer3.0.downsample.1.bias'],requires_grad=False)\n",
    "model_unet.layer3[0].downsample[1].running_mean = nn.Parameter(model_ww['layer3.0.downsample.1.running_mean'],requires_grad=False)\n",
    "model_unet.layer3[0].downsample[1].running_var = nn.Parameter(model_ww['layer3.0.downsample.1.running_var'],requires_grad=False)\n",
    "model_unet.layer3[0].downsample[1].num_batches_tracked = nn.Parameter(model_ww['layer3.0.downsample.1.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "\n",
    "model_unet.layer3[1].conv1.weight = nn.Parameter(model_ww['layer3.1.conv1.weight'],requires_grad=False)\n",
    "model_unet.layer3[1].bn1.weight = nn.Parameter(model_ww['layer3.1.bn1.weight'],requires_grad=False)\n",
    "model_unet.layer3[1].bn1.bias = nn.Parameter(model_ww['layer3.1.bn1.bias'],requires_grad=False)\n",
    "model_unet.layer3[1].bn1.running_mean = nn.Parameter(model_ww['layer3.1.bn1.running_mean'],requires_grad=False)\n",
    "model_unet.layer3[1].bn1.running_var = nn.Parameter(model_ww['layer3.1.bn1.running_var'],requires_grad=False)\n",
    "model_unet.layer3[1].bn1.num_batches_tracked = nn.Parameter(model_ww['layer3.1.bn1.num_batches_tracked'],requires_grad=False)\n",
    "model_unet.layer3[1].conv2.weight = nn.Parameter(model_ww['layer3.1.conv2.weight'],requires_grad=False)\n",
    "model_unet.layer3[1].bn2.weight = nn.Parameter(model_ww['layer3.1.bn2.weight'],requires_grad=False)\n",
    "model_unet.layer3[1].bn2.bias = nn.Parameter(model_ww['layer3.1.bn2.bias'],requires_grad=False)\n",
    "model_unet.layer3[1].bn2.running_mean = nn.Parameter(model_ww['layer3.1.bn2.running_mean'],requires_grad=False)\n",
    "model_unet.layer3[1].bn2.running_var = nn.Parameter(model_ww['layer3.1.bn2.running_var'],requires_grad=False)\n",
    "model_unet.layer3[1].bn2.num_batches_tracked = nn.Parameter(model_ww['layer3.1.bn2.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "model_unet.layer3_1x1[0].weight = nn.Parameter(model_ww['layer3_1x1.0.weight'],requires_grad=False)\n",
    "model_unet.layer3_1x1[0].bias = nn.Parameter(model_ww['layer3_1x1.0.bias'],requires_grad=False)\n",
    "\n",
    "\n",
    "model_unet.layer4[0].conv1.weight = nn.Parameter(model_ww['layer4.0.conv1.weight'],requires_grad=False)\n",
    "model_unet.layer4[0].bn1.weight = nn.Parameter(model_ww['layer4.0.bn1.weight'],requires_grad=False)\n",
    "model_unet.layer4[0].bn1.bias = nn.Parameter(model_ww['layer4.0.bn1.bias'],requires_grad=False)\n",
    "model_unet.layer4[0].bn1.running_mean = nn.Parameter(model_ww['layer4.0.bn1.running_mean'],requires_grad=False)\n",
    "model_unet.layer4[0].bn1.running_var = nn.Parameter(model_ww['layer4.0.bn1.running_var'],requires_grad=False)\n",
    "model_unet.layer4[0].bn1.num_batches_tracked = nn.Parameter(model_ww['layer4.0.bn1.num_batches_tracked'],requires_grad=False)\n",
    "model_unet.layer4[0].conv2.weight = nn.Parameter(model_ww['layer4.0.conv2.weight'],requires_grad=False)\n",
    "model_unet.layer4[0].bn2.weight = nn.Parameter(model_ww['layer4.0.bn2.weight'],requires_grad=False)\n",
    "model_unet.layer4[0].bn2.bias = nn.Parameter(model_ww['layer4.0.bn2.bias'],requires_grad=False)\n",
    "model_unet.layer4[0].bn2.running_mean = nn.Parameter(model_ww['layer4.0.bn2.running_mean'],requires_grad=False)\n",
    "model_unet.layer4[0].bn2.running_var = nn.Parameter(model_ww['layer4.0.bn2.running_var'],requires_grad=False)\n",
    "model_unet.layer4[0].bn2.num_batches_tracked = nn.Parameter(model_ww['layer4.0.bn2.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "model_unet.layer4[0].downsample[0].weight = nn.Parameter(model_ww['layer4.0.downsample.0.weight'],requires_grad=False)\n",
    "model_unet.layer4[0].downsample[1].weight = nn.Parameter(model_ww['layer4.0.downsample.1.weight'],requires_grad=False)\n",
    "model_unet.layer4[0].downsample[1].bias = nn.Parameter(model_ww[ 'layer4.0.downsample.1.bias'],requires_grad=False)\n",
    "model_unet.layer4[0].downsample[1].running_mean = nn.Parameter(model_ww['layer4.0.downsample.1.running_mean'],requires_grad=False)\n",
    "model_unet.layer4[0].downsample[1].running_var = nn.Parameter(model_ww['layer4.0.downsample.1.running_var'],requires_grad=False)\n",
    "model_unet.layer4[0].downsample[1].num_batches_tracked = nn.Parameter(model_ww['layer4.0.downsample.1.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "\n",
    "model_unet.layer4[1].conv1.weight = nn.Parameter(model_ww['layer4.1.conv1.weight'],requires_grad=False)\n",
    "model_unet.layer4[1].bn1.weight = nn.Parameter(model_ww['layer4.1.bn1.weight'],requires_grad=False)\n",
    "model_unet.layer4[1].bn1.bias = nn.Parameter(model_ww['layer4.1.bn1.bias'],requires_grad=False)\n",
    "model_unet.layer4[1].bn1.running_mean = nn.Parameter(model_ww['layer4.1.bn1.running_mean'],requires_grad=False)\n",
    "model_unet.layer4[1].bn1.running_var = nn.Parameter(model_ww['layer4.1.bn1.running_var'],requires_grad=False)\n",
    "model_unet.layer4[1].bn1.num_batches_tracked = nn.Parameter(model_ww['layer4.1.bn1.num_batches_tracked'],requires_grad=False)\n",
    "model_unet.layer4[1].conv2.weight = nn.Parameter(model_ww['layer4.1.conv2.weight'],requires_grad=False)\n",
    "model_unet.layer4[1].bn2.weight = nn.Parameter(model_ww['layer4.1.bn2.weight'],requires_grad=False)\n",
    "model_unet.layer4[1].bn2.bias = nn.Parameter(model_ww['layer4.1.bn2.bias'],requires_grad=False)\n",
    "model_unet.layer4[1].bn2.running_mean = nn.Parameter(model_ww['layer4.1.bn2.running_mean'],requires_grad=False)\n",
    "model_unet.layer4[1].bn2.running_var = nn.Parameter(model_ww['layer4.1.bn2.running_var'],requires_grad=False)\n",
    "model_unet.layer4[1].bn2.num_batches_tracked = nn.Parameter(model_ww['layer4.1.bn2.num_batches_tracked'],requires_grad=False)\n",
    "\n",
    "model_unet.layer4_1x1[0].weight = nn.Parameter(model_ww['layer4_1x1.0.weight'],requires_grad=False)\n",
    "model_unet.layer4_1x1[0].bias = nn.Parameter(model_ww['layer4_1x1.0.bias'],requires_grad=False)\n",
    "\n",
    "model_unet.conv_up3[0].weight = nn.Parameter(model_ww['conv_up3.0.weight'],requires_grad=False)\n",
    "model_unet.conv_up3[0].bias = nn.Parameter(model_ww['conv_up3.0.bias'],requires_grad=False)\n",
    "model_unet.conv_up2[0].weight = nn.Parameter(model_ww['conv_up2.0.weight'],requires_grad=False)\n",
    "model_unet.conv_up2[0].bias = nn.Parameter(model_ww['conv_up2.0.bias'],requires_grad=False)\n",
    "model_unet.conv_up1[0].weight = nn.Parameter(model_ww['conv_up1.0.weight'],requires_grad=False)\n",
    "model_unet.conv_up1[0].bias = nn.Parameter(model_ww['conv_up1.0.bias'],requires_grad=False)\n",
    "model_unet.conv_up0[0].weight = nn.Parameter(model_ww['conv_up0.0.weight'],requires_grad=False)\n",
    "model_unet.conv_up0[0].bias = nn.Parameter(model_ww['conv_up0.0.bias'],requires_grad=False)\n",
    "\n",
    "\n",
    "model_unet.conv_original_size0[0].weight = nn.Parameter(model_ww['conv_original_size0.0.weight'],requires_grad=False)\n",
    "model_unet.conv_original_size0[0].bias = nn.Parameter(model_ww['conv_original_size0.0.bias'],requires_grad=False)\n",
    "model_unet.conv_original_size1[0].weight = nn.Parameter(model_ww['conv_original_size1.0.weight'],requires_grad=False)\n",
    "model_unet.conv_original_size1[0].bias = nn.Parameter(model_ww['conv_original_size1.0.bias'],requires_grad=False)\n",
    "model_unet.conv_original_size2[0].weight = nn.Parameter(model_ww['conv_original_size2.0.weight'],requires_grad=False)\n",
    "model_unet.conv_original_size2[0].bias = nn.Parameter(model_ww['conv_original_size2.0.bias'],requires_grad=False)\n",
    "\n",
    "model_unet.conv_last.weight = nn.Parameter(model_ww['conv_last.weight'],requires_grad=False)\n",
    "model_unet.conv_last.bias = nn.Parameter(model_ww['conv_last.bias'],requires_grad=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_unet.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list1 = [\n",
    "        'sub-wlsubj001',\n",
    "        'sub-wlsubj004',\n",
    "        'sub-wlsubj006',\n",
    "        'sub-wlsubj007',\n",
    "        'sub-wlsubj014',\n",
    "        'sub-wlsubj019',\n",
    "        'sub-wlsubj023',\n",
    "        'sub-wlsubj042',\n",
    "        'sub-wlsubj043',\n",
    "        'sub-wlsubj045',\n",
    "        'sub-wlsubj046',\n",
    "        'sub-wlsubj055',\n",
    "        'sub-wlsubj056',\n",
    "        'sub-wlsubj057',\n",
    "        'sub-wlsubj062',\n",
    "        'sub-wlsubj064',\n",
    "        'sub-wlsubj067']\n",
    "\n",
    "subject_list2 = [\n",
    "        'sub-wlsubj071',\n",
    "        'sub-wlsubj076',\n",
    "        'sub-wlsubj079',\n",
    "        'sub-wlsubj081',\n",
    "        'sub-wlsubj083',\n",
    "        'sub-wlsubj084',\n",
    "        'sub-wlsubj085',\n",
    "        'sub-wlsubj086',\n",
    "        'sub-wlsubj087',\n",
    "        'sub-wlsubj088',\n",
    "        'sub-wlsubj090',\n",
    "        'sub-wlsubj091',\n",
    "        'sub-wlsubj092',\n",
    "        'sub-wlsubj094',\n",
    "        'sub-wlsubj095',\n",
    "        'sub-wlsubj104',\n",
    "        'sub-wlsubj105',\n",
    "        'sub-wlsubj109',\n",
    "        'sub-wlsubj114',\n",
    "        'sub-wlsubj115',\n",
    "        'sub-wlsubj116',\n",
    "        'sub-wlsubj117',\n",
    "        'sub-wlsubj118',\n",
    "        'sub-wlsubj120',\n",
    "#        'sub-wlsubj121',\n",
    "        'sub-wlsubj122',\n",
    "        'sub-wlsubj126']\n",
    "\n",
    "feature_list = ['x', 'y', 'z','curvature', 'convexity','thickness', 'surface_area','prf_x','prf_y','prf_sigma','prf_cod']\n",
    "rescale_feature_list = ['x', 'y', 'z','curvature', 'convexity','thickness', 'surface_area','prf_scaled_x','prf_scaled_y','prf_sigma','prf_cod']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bce loss (copy paste from visual autolabel.util._core.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_logits(data):\n",
    "    \"\"\"Attempts to guess whether the given PyTorch tensor contains logits.\n",
    "\n",
    "    If the argument `data` contains only values that are no less than 0 and no\n",
    "    greater than 1, then `False` is returned; otherwise, `True` is returned.\n",
    "    \"\"\"\n",
    "    if   (data > 1).any(): return True\n",
    "    elif (data < 0).any(): return True\n",
    "    else:                  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, gold, logits=None, smoothing=1, graph=False, metrics=None):\n",
    "    \"\"\"Returns the loss based on the dice coefficient.\n",
    "    \n",
    "    `dice_loss(pred, gold)` returns the dice-coefficient loss between the\n",
    "    tensors `pred` and `gold` which must be the same shape and which should\n",
    "    represent probabilities. The first two dimensions of both `pred` and `gold`\n",
    "    must represent the batch-size and the classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : tensor\n",
    "        The predicted probabilities of each class.\n",
    "    gold : tensor\n",
    "        The gold-standard labels for each class.\n",
    "    logits : boolean, optional\n",
    "        Whether the values in `pred` are logits--i.e., unnormalized scores that\n",
    "        have not been run through a sigmoid calculation already. If this is\n",
    "        `True`, then the BCE starts by calculating the sigmoid of the `pred`\n",
    "        argument. If `None`, then attempts to deduce whether the input is or is\n",
    "        not logits. The default is `None`.\n",
    "    smoothing : number, optional\n",
    "        The smoothing coefficient `s`. The default is `1`.\n",
    "    metrics : dict or None, optional\n",
    "        An optional dictionary into which the key `'dice'` should be inserted\n",
    "        with the dice-loss as the value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The dice-coefficient loss of the prediction.\n",
    "    \"\"\"\n",
    "    pred = pred.contiguous()\n",
    "    gold = gold.contiguous()\n",
    "    if logits is None: logits = is_logits(pred)\n",
    "    if logits: pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * gold)\n",
    "    pred = pred**2\n",
    "    gold = gold**2\n",
    "    while len(intersection.shape) > 2:\n",
    "        intersection = intersection.sum(dim=-1)\n",
    "        pred = pred.sum(dim=-1)\n",
    "        gold = gold.sum(dim=-1)\n",
    "    if smoothing is None: smoothing = 0\n",
    "    loss = (1 - ((2 * intersection + smoothing) / (pred + gold + smoothing)))\n",
    "    # Average the loss across classes then take the mean across batch elements.\n",
    "    loss = loss.mean(dim=1).mean()\n",
    "    if metrics is not None:\n",
    "        if 'dice' not in metrics: metrics['dice'] = 0.0\n",
    "        metrics['dice'] += loss.data.cpu().numpy() * gold.size(0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_list = ['V1','V2','V3']\n",
    "loss_sum = torch.ones(len(subject_list2),3)\n",
    "\n",
    "ots = - 1 \n",
    "for sub_name in subject_list2:\n",
    "    ots += 1\n",
    "    pred_all = np.load('func_result/' + sub_name + '.npy')\n",
    "    for ii in range(3):\n",
    "        pred = torch.from_numpy(pred_all[0,ii,:,:].squeeze())\n",
    "        true_val = retinotopy_cache.get(sub_name,cor_list[ii],multiproc= False)\n",
    "        true_val = resize_tensor(true_val.unsqueeze(dim=0).unsqueeze(dim=0),(128,256)).squeeze()   \n",
    "        dice_l = dice_loss(pred,true_val) \n",
    "        loss_sum[ots,ii] = dice_l\n",
    "        del true_val\n",
    "        \n",
    "torch.save(loss_sum,'func_result/loss2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
