{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual-Area Autolabeler: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Noah C. Benson](mailto:nben@uw.edu)$^{1,2,3}$, [Shaoling Chen](mailto:sc6995@nyu.edu)$^{4}$, and [Jonathan Winawer](mailto:jonathan.winawer@nyu.edu)$^{1,2}$\n",
    "\n",
    "$^1$Department of Psychology  \n",
    "$^2$Center for Neural Sciences  \n",
    "$^4$Courant Institute for Mathematics  \n",
    "New York University  \n",
    "New York, NY 10012\n",
    "\n",
    "$^3$**Current Affiliation:**  \n",
    "eScience Institute  \n",
    "University of Washington  \n",
    "Seattle, WA 98195"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define any configuration item that needs to be set locally for the system running this notebook. Most likely, you will have to edit these in order for the model to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_cache_path\n",
    "# The directory into which data for the model training should be cached. This\n",
    "# can be None, but if it is, then the training images will need to be\n",
    "# regenerated every time the notebook is run.\n",
    "data_cache_path  = '/data/visual-autolabel/data'\n",
    "\n",
    "# model_cache_path\n",
    "# The directory into which to store models that are generated during training.\n",
    "# This may be None, but if it is, then the best models will not be saved out to\n",
    "# disk during rounds of training.\n",
    "model_cache_path = '/data/visual-autolabel/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pimms, pandas, json\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import nibabel as nib\n",
    "import pyrsistent as pyr\n",
    "import neuropythy as ny\n",
    "import torch, torchvision, torchsummary\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import ipyvolume as ipv\n",
    "\n",
    "import visual_autolabel as va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional matplotlib preferences; these are just display preferences.\n",
    "mpl_font_config = {'family':'sans-serif',\n",
    "                   'sans-serif':['HelveticaNeue', 'Helvetica', 'Arial'],\n",
    "                   'size': 10,\n",
    "                   'weight': 'light'}\n",
    "mpl.rc('font', **mpl_font_config)\n",
    "# We want relatively high-res images, especially when saving to disk.\n",
    "mpl.rcParams['figure.dpi'] = 72*4\n",
    "mpl.rcParams['savefig.dpi'] = 72*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_until(training_plan, until=None, model_key=None,\n",
    "                model_cache_path=model_cache_path,\n",
    "                data_cache_path=data_cache_path,\n",
    "                create_directories=True,\n",
    "                create_mode=0o755):\n",
    "    \"\"\"Continuously runs the given training plan for models until an interrupt.\n",
    "        \n",
    "    Runs training on `'anat'`, `'func'`, and `'both'` models, sequentially,\n",
    "    using random partitions until a keyboard interrupt is caught, at which\n",
    "    point a `pandas` dataframe of the results is returned. The partition is\n",
    "    generated only once per group of model trainings (i.e., per training of an\n",
    "    anatomical, functional, and combined model).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_plan : list of dicts\n",
    "        The training-plan to pass to the `visual_autolabel.run_modelplan()`\n",
    "        function.\n",
    "    model_key : str or None, optional\n",
    "        A string that should be appended, as a sub-directory name, to the\n",
    "        `model_cache_path`; this argument allows one to save model training\n",
    "        to a specific sub-directory of the `model_cache_path` directory.\n",
    "    model_cache_path : str, optional\n",
    "        The cache-path to use for the model training. By default, this is the\n",
    "        global variable `model_cache_path`, defined above.\n",
    "    data_cache_path : str, optional\n",
    "        The cache-path from which data for the model training should be loaded.\n",
    "        By default, this is the global variable `data_cache_path`, defined\n",
    "        above.\n",
    "    until : int or None, optional\n",
    "        If an integer is provided, then only `until` groups of trainings are\n",
    "        performed, then the result is returned. If `None`, then the training\n",
    "        continues until a `KeyboardInterrupt` is caught. The default is `None`.\n",
    "    create_directories : boolean, optional\n",
    "        Whether to create cache directories that do not exist (default `True`).\n",
    "    create_mode : int, optional\n",
    "        What mode to use when creating directories (default: `0o755`).\n",
    "    \"\"\"\n",
    "    if data_cache_path is Ellipsis:\n",
    "        data_cache_path = globals()['data_cache_path']\n",
    "    if model_cache_path is Ellipsis:\n",
    "        model_cache_path = globals()['model_cache_path']\n",
    "    if model_key is not None:\n",
    "        if model_cache_path is None:\n",
    "            model_cache_path = model_key\n",
    "        else:\n",
    "            model_cache_path = os.path.join(model_cache_path, model_key)\n",
    "    if not os.path.isdir(model_cache_path) and create_directories:\n",
    "        os.makedirs(model_cache_path, create_mode)\n",
    "    if not os.path.isdir(data_cache_path) and create_directories:\n",
    "        os.makedirs(data_cache_path, create_mode)\n",
    "    training_history = []\n",
    "    datatype_tr = dict(anat='Anatomical Data Only',\n",
    "                       func='Functional Data Only',\n",
    "                       both='Anatomical & Functional Data')\n",
    "    try:\n",
    "        print('')\n",
    "        iterno = 0\n",
    "        while True:\n",
    "            if until is not None and iterno >= until: break\n",
    "            iterno += 1\n",
    "            # Make one partition for all three minimization types.\n",
    "            part = va.partition(va.sids, how=(0.8, 0.2))\n",
    "            pid = va.partition_id(part)\n",
    "            print('%-15s%70s' % ('Iteration %d' % iterno,\n",
    "                                 'Partition ID: %s' % pid))\n",
    "            print('=' * 85)\n",
    "            for (dtype,dnm) in datatype_tr.items():\n",
    "                print('')\n",
    "                print(dnm + ' ' + '-'*(85 - len(dnm) - 1))\n",
    "                print('')\n",
    "                t0 = time.time()\n",
    "                (model, loss, dice) = va.train.run_modelplan(\n",
    "                    training_plan,\n",
    "                    partition=part,\n",
    "                    features=dtype,\n",
    "                    model_cache_path=model_cache_path,\n",
    "                    data_cache_path=data_cache_path)\n",
    "                t1 = time.time()\n",
    "                row = dict(input=dtype, loss=loss, dice=dice,\n",
    "                           training_time=(t1-t0))\n",
    "                training_history.append(row)\n",
    "                print('')\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('KeyboardInterrupt caught; ending training.')\n",
    "    training_history = ny.to_dataframe(training_history)\n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(dataset, k, model,\n",
    "                    axes=None, figsize=(6,1), dpi=72*4,\n",
    "                    min_alpha=0.5,\n",
    "                    channels=(0,1,4,5),\n",
    "                    round_labels=True):\n",
    "    \"\"\"Plots the data, true label, and predicted label (by model) of a dataset.\n",
    "    \n",
    "    `plot_prediction(dataset, k, model)` creates a `matplotlib` figure for\n",
    "    `dataset[k]` (i.e., the `k`th subject/image in `dataset`). The `axes` are\n",
    "    always returned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : HCPVisualDataset\n",
    "        The dataset used to plot the predictions. This may alternately be a\n",
    "        PyTorch dataloader, in which case the dataloader's dataset must be an\n",
    "        `HCPVisualDataset` object.\n",
    "    k : int\n",
    "        The sample number or subject ID to plot. A sample number is just the\n",
    "        index number for the subject in the dataset; if a number less than 1000\n",
    "        is given, then it is assumed to eb a subject index, while if it is over\n",
    "        1000, it is assumed to be a subject ID.\n",
    "    model : PyTorch Module\n",
    "        A UNet model or other PyTorch model that makes a segmentation\n",
    "        of the images from the given `dataset`.\n",
    "    axes : MatPlotLib axes or `None`, optional\n",
    "        A set of axes onto which to plot the predictions. Must have a total\n",
    "        flattened length of 3.\n",
    "    figsize : tuple, optional\n",
    "        A tuple of `(width, height)` in inches to use for the figure size. This\n",
    "        is ignored if `axes` is provided. The default is `(6, 1)`.\n",
    "    dpi : int, optional\n",
    "        The number of dots per inch in the output image. If `axes` is provided,\n",
    "        this option is ignored. The default is `72 * 4`.\n",
    "    min_alpha : float, optional\n",
    "        The minimum alpha value to show in the alpha channel of the image.\n",
    "        Values below this level are replaced by the formula\n",
    "        `adjusted_value = value * (1 - min_alpha) + min_alpha`. The default is\n",
    "        `0.5`.\n",
    "    channels : iterable of ints, optional\n",
    "        When a dataset whose input images have more than 4 image channels is\n",
    "        provided (i.e., the `'both'` datasets, which have 4 anatomical and 4\n",
    "        functional image layers), then this list of 4 channels is used. By\n",
    "        default this is `(0,1,4,5)`. This option is ignored if the dataset\n",
    "        contains images with only 4 channels.\n",
    "    round_labels : boolean, optional\n",
    "        Whether to round every channel to either 1 or 0 before plotting the\n",
    "        labels. The default is `True`.\n",
    "    \"\"\"\n",
    "    if k > 1000:\n",
    "        # We have a subject-ID instead of an index.\n",
    "        k = np.where(dataset.sids == k)[0]\n",
    "    (imdat, imlbl) = dataset[k]\n",
    "    impre = model(imdat[None,:,:,:].float())\n",
    "    if not model.apply_sigmoid:\n",
    "        impre = torch.sigmoid(impre)\n",
    "    impre = dataset.inv_transform(None, impre.detach()[0])\n",
    "    (imdat, imlbl) = dataset.inv_transform(imdat, imlbl)\n",
    "    if axes is None:\n",
    "        (fig,axes) = plt.subplots(1, 3, figsize=figsize, dpi=dpi)\n",
    "    # with imdat we want to adjust the alpha layer\n",
    "    imdat = np.array(imdat)\n",
    "    imdat[:,:,3] = imdat[:,:,3]*(1 - min_alpha) + min_alpha\n",
    "    for (ax,im,rl) in zip(axes, [imdat,imlbl,impre], [0,0,round_labels]):\n",
    "        if im.shape[2] > 4: im = im[:,:,:4]\n",
    "        im = np.clip(im, 0, 1)\n",
    "        if rl:\n",
    "            z = im < 0.5\n",
    "            im[z] = 0\n",
    "            im[~z] = 1\n",
    "        ax.imshow(im)\n",
    "        ax.axis('off')\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define our standard training plan for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_plan = [\n",
    "    dict(lr=0.00375, gamma=0.9, num_epochs=10,  bce_weight=0.67),\n",
    "    dict(lr=0.00250, gamma=0.9, num_epochs=10,  bce_weight=0.33),\n",
    "    dict(lr=0.00125, gamma=0.9, num_epochs=10,  bce_weight=0.00)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we define a standard set of training and validation subjects. If the files `training_sids.json` and `validate_sids.json` are found in the current directory, they are imported; otherwise, a random partition is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.isfile('training_sids.json') and\n",
    "    os.path.isfile('validate_sids.json')):\n",
    "    with open('training_sids.json', 'r') as fl:\n",
    "        trn_sids = np.array(json.load(fl))\n",
    "    with open('validate_sids.json', 'r') as fl:\n",
    "        val_sids = np.array(json.load(fl))\n",
    "else:\n",
    "    (trn_sids, val_sids) = va.partition(va.sids)\n",
    "    # The following can be used to save these out to files.\n",
    "    #with open('training_sids.json', 'w') as fl:\n",
    "    #    json.dump(trn_sids.tolist(), fl)\n",
    "    #with open('validate_sids.json', 'w') as fl:\n",
    "    #    json.dump(val_sids.tolist(), fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "Training: anat\n",
      "\n",
      "epoch    lr     dt[s]   trn bce   trn dice  trn loss   val bce   val dice  val loss\n",
      "=====--=======--=====-+-========--========--========-+-========--========--========\n",
      " 1/10  0.00375   50.9 |   12.367     0.888     8.579 |    0.426     0.890     0.579 *\n",
      " 2/10  0.00337   13.3 |    0.302     0.658     0.419 |    0.301     0.699     0.433 *\n",
      " 3/10  0.00304   17.2 |    0.245     0.530     0.339 |    0.201     0.444     0.281 *\n",
      " 4/10  0.00273   21.9 |    0.196     0.409     0.266 |    0.191     0.373     0.251 *\n",
      " 5/10  0.00246   24.5 |    0.174     0.345     0.230 |    0.182     0.350     0.237 *\n",
      " 6/10  0.00221   23.2 |    0.156     0.311     0.207 |    0.217     0.345     0.259 \n",
      " 7/10  0.00199   20.6 |    0.150     0.290     0.196 |    0.144     0.282     0.189 *\n",
      " 8/10  0.00179   20.8 |    0.142     0.268     0.184 |    0.172     0.328     0.223 \n",
      " 9/10  0.00161   20.8 |    0.139     0.260     0.178 |    0.164     0.295     0.207 \n",
      "10/10  0.00145   21.0 |    0.141     0.266     0.182 |    0.132     0.241     0.168 *\n",
      "Best val loss: 0.167637\n",
      "\n",
      "epoch    lr     dt[s]   trn bce   trn dice  trn loss   val bce   val dice  val loss\n",
      "=====--=======--=====-+-========--========--========-+-========--========--========\n",
      " 1/10  0.00250   20.3 |    0.549     0.538     0.542 |    0.440     0.776     0.665 *\n",
      " 2/10  0.00225   20.9 |    0.194     0.344     0.294 |    0.186     0.382     0.317 *\n",
      " 3/10  0.00203   20.2 |    0.175     0.306     0.263 |    0.185     0.314     0.271 *\n",
      " 4/10  0.00182   20.2 |    0.153     0.273     0.234 |    0.144     0.248     0.213 *\n",
      " 5/10  0.00164   20.5 |    0.143     0.244     0.211 |    0.175     0.294     0.255 \n",
      " 6/10  0.00148   20.4 |    0.134     0.224     0.194 |    0.197     0.321     0.280 \n",
      " 7/10  0.00133   20.9 |    0.126     0.207     0.180 |    0.169     0.287     0.248 \n",
      " 8/10  0.00120   20.9 |    0.128     0.209     0.182 |    0.121     0.197     0.172 *\n",
      " 9/10  0.00108   20.8 |    0.123     0.200     0.174 |    0.128     0.215     0.186 \n",
      "10/10  0.00097   20.7 |    0.121     0.195     0.171 |    0.112     0.185     0.161 *\n",
      "Best val loss: 0.160661\n",
      "\n",
      "epoch    lr     dt[s]   trn bce   trn dice  trn loss   val bce   val dice  val loss\n",
      "=====--=======--=====-+-========--========--========-+-========--========--========\n",
      " 1/10  0.00125   18.2 |    0.168     0.227     0.227 |    0.133     0.191     0.191 *\n",
      " 2/10  0.00113   17.5 |    0.137     0.187     0.187 |    0.141     0.200     0.200 \n",
      " 3/10  0.00101   18.0 |    0.144     0.192     0.192 |    0.157     0.216     0.216 \n",
      " 4/10  0.00091   17.8 |    0.148     0.189     0.189 |    0.132     0.189     0.189 *\n",
      " 5/10  0.00082   17.5 |    0.136     0.179     0.179 |    0.152     0.207     0.207 \n",
      " 6/10  0.00074   18.1 |    0.132     0.176     0.176 |    0.130     0.174     0.174 *\n",
      " 7/10  0.00066   18.5 |    0.132     0.175     0.175 |    0.152     0.213     0.213 \n",
      " 8/10  0.00060   19.8 |    0.126     0.166     0.166 |    0.135     0.177     0.177 \n",
      " 9/10  0.00054   20.3 |    0.120     0.163     0.163 |    0.183     0.282     0.282 \n",
      "10/10  0.00048   20.3 |    0.123     0.162     0.162 |    0.133     0.187     0.187 \n",
      "Best val loss: 0.173598\n",
      "\n",
      "=====================================================================================\n",
      "Training: func\n",
      "\n",
      "epoch    lr     dt[s]   trn bce   trn dice  trn loss   val bce   val dice  val loss\n",
      "=====--=======--=====-+-========--========--========-+-========--========--========\n",
      " 1/10  0.00375   48.9 |   33.047     0.888    22.434 |    0.346     0.692     0.460 *\n",
      " 2/10  0.00337   12.7 |    0.327     0.690     0.447 |    0.297     0.672     0.421 *\n",
      " 3/10  0.00304   12.8 |    0.292     0.648     0.410 |    0.291     0.601     0.393 *\n",
      " 4/10  0.00273   12.7 |    0.261     0.549     0.356 |    0.248     0.479     0.324 *\n",
      " 5/10  0.00246   12.7 |    0.224     0.458     0.302 |    0.203     0.413     0.273 *\n",
      " 6/10  0.00221   12.7 |    0.198     0.387     0.260 |    0.203     0.342     0.249 *\n",
      " 7/10  0.00199   12.7 |    0.179     0.332     0.230 |    0.165     0.326     0.218 *\n",
      " 8/10  0.00179   12.8 |    0.163     0.301     0.209 |    0.158     0.277     0.197 *\n",
      " 9/10  0.00161   12.9 |    0.153     0.273     0.192 |    0.147     0.254     0.182 *\n",
      "10/10  0.00145   12.9 |    0.145     0.255     0.181 |    0.140     0.267     0.182 *\n",
      "Best val loss: 0.181913\n",
      "\n",
      "epoch    lr     dt[s]   trn bce   trn dice  trn loss   val bce   val dice  val loss\n",
      "=====--=======--=====-+-========--========--========-+-========--========--========\n",
      " 1/10  0.00250   12.6 |    1.079     0.675     0.808 |    0.302     0.593     0.497 *\n",
      " 2/10  0.00225   12.6 |    0.289     0.558     0.469 |    0.296     0.507     0.437 *\n",
      " 3/10  0.00203   12.6 |    0.246     0.459     0.389 |    0.232     0.417     0.356 *\n",
      " 4/10  0.00182   12.8 |    0.203     0.357     0.307 |    0.170     0.277     0.242 *\n",
      " 5/10  0.00164   12.8 |    0.169     0.273     0.239 |    0.142     0.233     0.203 *\n",
      " 6/10  0.00148   12.8 |    0.144     0.234     0.204 |    0.136     0.217     0.190 *\n",
      " 7/10  0.00133   12.6 |    0.134     0.214     0.188 |    0.142     0.216     0.191 \n",
      " 8/10  0.00120   12.6 |    0.131     0.205     0.181 |    0.137     0.216     0.190 *\n",
      " 9/10  0.00108   13.0 |    0.124     0.194     0.171 |    0.120     0.186     0.164 *\n",
      "10/10  0.00097   12.8 |    0.115     0.181     0.159 |    0.113     0.178     0.157 *\n",
      "Best val loss: 0.156518\n",
      "\n",
      "epoch    lr     dt[s]   trn bce   trn dice  trn loss   val bce   val dice  val loss\n",
      "=====--=======--=====-+-========--========--========-+-========--========--========\n",
      " 1/10  0.00125   13.1 |    0.147     0.218     0.218 |    0.123     0.192     0.192 *\n",
      " 2/10  0.00113   12.9 |    0.126     0.187     0.187 |    0.127     0.191     0.191 *\n",
      " 3/10  0.00101   12.9 |    0.116     0.167     0.167 |    0.116     0.165     0.165 *\n",
      " 4/10  0.00091   12.9 |    0.109     0.156     0.156 |    0.111     0.161     0.161 *\n",
      " 5/10  0.00082   13.0 |    0.105     0.149     0.149 |    0.107     0.150     0.150 *\n",
      " 6/10  0.00074   13.0 |    0.104     0.145     0.145 |    0.129     0.160     0.160 \n",
      " 7/10  0.00066   13.1 |    0.102     0.139     0.139 |    0.100     0.148     0.148 *\n",
      " 8/10  0.00060   13.2 |    0.095     0.130     0.130 |    0.098     0.134     0.134 *\n",
      " 9/10  0.00054   13.0 |    0.091     0.123     0.123 |    0.098     0.135     0.135 \n",
      "10/10  0.00048   13.1 |    0.089     0.121     0.121 |    0.101     0.136     0.136 \n",
      "Best val loss: 0.133641\n",
      "\n",
      "=====================================================================================\n",
      "Training: both\n",
      "\n",
      "epoch    lr     dt[s]   trn bce   trn dice  trn loss   val bce   val dice  val loss\n",
      "=====--=======--=====-+-========--========--========-+-========--========--========\n",
      " 1/10  0.00375   49.1 |   40.433     0.814    27.359 |    0.485     0.861     0.609 *\n",
      " 2/10  0.00337   12.8 |    0.265     0.548     0.358 |    0.202     0.396     0.266 *\n",
      " 3/10  0.00304   12.9 |    0.187     0.376     0.250 |    0.187     0.415     0.262 *\n",
      " 4/10  0.00273   12.7 |    0.153     0.295     0.200 |    0.124     0.220     0.156 *\n",
      " 5/10  0.00246   12.8 |    0.122     0.220     0.155 |    0.129     0.225     0.161 \n",
      " 6/10  0.00221   13.0 |    0.114     0.202     0.143 |    0.103     0.181     0.129 *\n",
      " 7/10  0.00199   12.8 |    0.105     0.181     0.130 |    0.096     0.162     0.118 *\n",
      " 8/10  0.00179   12.8 |    0.096     0.161     0.117 |    0.085     0.142     0.103 *\n",
      " 9/10  0.00161   12.8 |    0.091     0.151     0.110 |    0.087     0.142     0.105 \n",
      "10/10  0.00145   12.8 |    0.089     0.146     0.108 |    0.080     0.130     0.097 *\n",
      "Best val loss: 0.096679\n",
      "\n",
      "epoch    lr     dt[s]   trn bce   trn dice  trn loss   val bce   val dice  val loss\n",
      "=====--=======--=====-+-========--========--========-+-========--========--========\n",
      " 1/10  0.00250   12.7 |    0.518     0.533     0.528 |    0.258     0.541     0.448 *\n",
      " 2/10  0.00225   12.7 |    0.136     0.233     0.201 |    0.181     0.352     0.295 *\n",
      " 3/10  0.00203   12.7 |    0.109     0.181     0.157 |    0.088     0.142     0.124 *\n",
      " 4/10  0.00182   12.8 |    0.099     0.163     0.142 |    0.089     0.139     0.122 *\n",
      " 5/10  0.00164   12.8 |    0.091     0.145     0.127 |    0.087     0.157     0.134 \n",
      " 6/10  0.00148   12.8 |    0.091     0.145     0.127 |    0.085     0.130     0.115 *\n",
      " 7/10  0.00133   12.9 |    0.084     0.131     0.115 |    0.077     0.121     0.107 *\n",
      " 8/10  0.00120   12.9 |    0.075     0.113     0.101 |    0.072     0.109     0.097 *\n",
      " 9/10  0.00108   12.9 |    0.071     0.104     0.093 |    0.070     0.101     0.091 *\n",
      "10/10  0.00097   13.1 |    0.069     0.099     0.089 |    0.069     0.100     0.090 *\n",
      "Best val loss: 0.089726\n",
      "\n",
      "epoch    lr     dt[s]   trn bce   trn dice  trn loss   val bce   val dice  val loss\n",
      "=====--=======--=====-+-========--========--========-+-========--========--========\n",
      " 1/10  0.00125   26.7 |    0.101     0.143     0.143 |    0.073     0.110     0.110 *\n",
      " 2/10  0.00113   30.3 |    0.069     0.096     0.096 |    0.069     0.097     0.097 *\n",
      " 3/10  0.00101   30.3 |    0.064     0.086     0.086 |    0.066     0.091     0.091 *\n",
      " 4/10  0.00091   30.2 |    0.061     0.080     0.080 |    0.061     0.084     0.084 *\n",
      " 5/10  0.00082   29.7 |    0.057     0.073     0.073 |    0.061     0.082     0.082 *\n",
      " 6/10  0.00074   28.0 |    0.061     0.079     0.079 |    0.058     0.077     0.077 *\n",
      " 7/10  0.00066   26.9 |    0.055     0.068     0.068 |    0.059     0.078     0.078 \n",
      " 8/10  0.00060   27.9 |    0.053     0.063     0.063 |    0.058     0.073     0.073 *\n",
      " 9/10  0.00054   28.4 |    0.052     0.063     0.063 |    0.056     0.074     0.074 \n",
      "10/10  0.00048   29.7 |    0.050     0.058     0.058 |    0.056     0.070     0.070 *\n",
      "Best val loss: 0.070147\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Basic options:\n",
    "opts = dict(partition=(trn_sids, val_sids),\n",
    "            model_cache_path=model_cache_path,\n",
    "            data_cache_path=data_cache_path)\n",
    "# We store the results in these dictionaries.\n",
    "models = {}\n",
    "losses = {}\n",
    "dice = {}\n",
    "# Run one modelplan for each feature type.\n",
    "for feat in ('anat', 'func', 'both'):\n",
    "    print(\"=\"*85)\n",
    "    print(f\"Training: {feat}\")\n",
    "    print(\"\")\n",
    "    r = va.run_modelplan(training_plan,\n",
    "                         features=feat,\n",
    "                         **opts)\n",
    "    print(\"\")\n",
    "    models[feat] = r[0]\n",
    "    losses[feat] = r[1]\n",
    "    dice[feat] = r[2]\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Train Continuously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we train the model continuously until an interruption is received (typically a keyboard interruption, which can be sent via the `Kernel > Interrupt Kernel` menu item in Jupyter. This slowly produces a lot of output, but the result, which is returned once the interrupt is sent, will be a `pandas` dataframe of training statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = train_until(training_plan, model_key='2022-04-15_01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#------------------------------------------------------------------------------\n",
    "# What feature-set do we want to plot?\n",
    "features = 'both'\n",
    "# What model are we plotting the results for?\n",
    "model = models[features]\n",
    "# Which subject-IDs (or subject indices, either one) are we plotting?\n",
    "plot_subs = np.arange(6)\n",
    "# Training or Validation data?\n",
    "phase = 'val'\n",
    "# What datasets are we using? (If None, then makes one using the default\n",
    "# partition).\n",
    "datasets = None\n",
    "\n",
    "# Make the Figure\n",
    "#------------------------------------------------------------------------------\n",
    "# Make the datasets if need be.\n",
    "if datasets is None:\n",
    "    datasets = va.make_datasets(partition=(trn_sids, val_sids),\n",
    "                                cache_path=data_cache_path)\n",
    "# Make the figure and axes that we're going to use.\n",
    "(fig,axs) = plt.subplots(len(plot_subs), 3, figsize=(6, len(plot_subs)), dpi=72*4)\n",
    "# And plot each row using the `plot_prediction` function.\n",
    "for (axrow,idx) in zip(axs, plot_subs):\n",
    "    plot_prediction(datasets[features][phase], idx, model, axes=axrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code needs to be checked and updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_model = trained_models['func'][0]\n",
    "anat_model = trained_models['anat'][0]\n",
    "\n",
    "func_ds = datasets['val']['func']\n",
    "anat_ds = datasets['val']['anat']\n",
    "\n",
    "def npdice(a,b):\n",
    "    a = torch.tensor(a.T[:3])\n",
    "    b = torch.tensor(b.T[:3])\n",
    "    return 1 - dice_loss(a[None,:,:,:], b[None,:,:,:])\n",
    "def subdice(ds, k, model):\n",
    "    sid = ds.sids[k]\n",
    "    (trnim, ansim) = ds[k]\n",
    "    preim = torch.sigmoid(model(trnim[None,:,:,:].float()))\n",
    "    preim = ds.inv_transform(None, preim.detach()[0])\n",
    "    (trnim, ansim) = ds.inv_transform(trnim, ansim)\n",
    "    infim = comparison_image(sid)\n",
    "    priim = companat_image()\n",
    "    return {'model': npdice(preim, ansim),\n",
    "            'bayes': npdice(infim, ansim),\n",
    "            'prior': npdice(priim, ansim)}\n",
    "def alldice(ds, model):\n",
    "    dat = [subdice(ds, k, model) for k in range(len(ds.sids))]\n",
    "    res = []\n",
    "    for (m,sid) in zip(dat, ds.sids):\n",
    "        m = {k:v.detach().numpy() for (k,v) in m.items()}\n",
    "        m['sid'] = sid\n",
    "        res.append(m)\n",
    "    return ny.util.to_dataframe(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = alldice(anat_ds, anat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': (0.7586430125225625,\n",
       "  0.008477384465692323,\n",
       "  0.7724644373568392,\n",
       "  0.7247349369185487,\n",
       "  0.8021125891625124),\n",
       " 'bayes': (0.7087995141025225,\n",
       "  0.009643316660442929,\n",
       "  0.7249986218068809,\n",
       "  0.6947594585447514,\n",
       "  0.7440246912721558),\n",
       " 'prior': (0.6983875593065093,\n",
       "  0.0077295654142980575,\n",
       "  0.7006973293733195,\n",
       "  0.6724228998098741,\n",
       "  0.7331302495354244)}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: (np.mean(v), np.std(v)/np.sqrt(len(v)), np.median(v), np.percentile(v, 25), np.percentile(v, 75))\n",
    " for k in ['model','bayes','prior']\n",
    " for v in [d[k].values]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code needs to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    'dataset': ['both'],\n",
    "    'batch_size': [2,5,8],\n",
    "    'lr': [0.01,0.005,0.00375,0.0025,0.00125],\n",
    "    'gamma': [0.95, 0.9, 0.75, 0.5, 0.25],\n",
    "    'pretrained_resnet': [True, False]}\n",
    "\n",
    "grid = []\n",
    "for (k,vs) in grid_params.items():\n",
    "    if len(grid) == 0:\n",
    "        grid = [{k:v} for v in vs]\n",
    "    else:\n",
    "        grid = [dict(u, **{k:v}) for u in grid for v in vs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "os.nice(10) # run these nicely!\n",
    "cpath0 = os.path.join(resnet_path, 'grid')\n",
    "\n",
    "for (ii,row) in enumerate(grid):\n",
    "    if ii % 12 == 0:\n",
    "        print(\"%3d of %-3d (%6.2f%%)\" % (ii+1, len(grid), ii/(len(grid)-1)*100))\n",
    "    tag = 'dat%04d' % (ii,)\n",
    "    cpath = os.path.join(cpath0, tag)\n",
    "    cfile = os.path.join(cpath0, tag + '.json')\n",
    "    if os.path.isfile(cfile):\n",
    "        for (k,v) in ny.load(cfile).items():\n",
    "            row[k] = v\n",
    "        continue\n",
    "    mdl0 = resnet_model(pretrained=row['pretrained'])\n",
    "    (mdl,loss,dice) = plan_to_model(mdl0, [row], cache_path=cpath, logger=None)\n",
    "    row['tag'] = tag\n",
    "    row['loss'] = loss\n",
    "    row['dice'] = dice\n",
    "    # Clear out the files that we don't need and resave the model and data\n",
    "    ny.save(cfile, row)\n",
    "    shutil.move(os.path.join(cpath,  'round01', 'optim000019.pkl'),\n",
    "                os.path.join(cpath0, 'opt%04d.pkl' % ii))\n",
    "    shutil.move(os.path.join(cpath,  'round01', 'model000019.pkl'),\n",
    "                os.path.join(cpath0, 'mdl%04d.pkl' % ii))\n",
    "    shutil.rmtree(cpath)    \n",
    "    \n",
    "grid = ny.to_dataframe(grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Noah's Python Environment",
   "language": "python",
   "name": "python-nben"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
